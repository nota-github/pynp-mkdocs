{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NetsPresso!","text":"<p>    \ud83e\udd1d Collaboration with partners \ud83e\udd1d  Qualcomm AI Hub x NetsPresso  STM32 x NetsPresso <p></p>    \u2600\ufe0f NetsPresso Model Zoo \u2600\ufe0f   YOLO Fastest      |  YOLOX      |  YOLOv8       |  YOLOv7       |  YOLOv5       |  PIDNet           |  PyTorch-CIFAR-Models <p></p>    \ud83d\udd25 NetsPresso Model Optimization Tutorials \ud83d\udd25   A Practical Guide to Using NetsPresso's Compressor Module   A Practical Guide to Using NetsPresso's Quantizer Module  <p></p> <p>Learn how to install and use the NetsPresso.</p> <p>Note</p> <p>You need an account to use the NetsPresso.</p> <p>\ud83d\udc49\ud83c\udffb Please create an account at NetsPresso</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Description</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>Join our Discussion Forum for providing feedback or sharing your use cases, and if you want to talk more with Nota, please contact us here. Or you can also do it via email(netspresso@nota.ai) or phone(+82 2-555-8659)!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python\u00a0<code>3.8</code>\u00a0|\u00a0<code>3.9</code>\u00a0|\u00a0<code>3.10</code></li> <li>PyTorch\u00a0<code>1.13.0</code>\u00a0(recommended) (compatible with:\u00a0<code>1.11.x</code>\u00a0-\u00a0<code>1.13.x</code>)</li> <li>TensorFlow <code>2.8.0</code> (recommended) (compatible with:\u00a0<code>2.3.x</code>\u00a0-\u00a0<code>2.8.x</code>)</li> </ul>"},{"location":"installation/#install-with-pypi-stable","title":"Install with PyPI (stable)","text":"<pre><code>pip install netspresso\n</code></pre>"},{"location":"installation/#install-with-github","title":"Install with GitHub","text":"<p>To install with editable mode,</p> <pre><code>git clone https://github.com/nota-netspresso/pynetspresso.git\ncd pynetspresso\npip install -e .\n</code></pre>"},{"location":"installation/#install-with-docker","title":"Install with Docker","text":"<p>Please clone this repository and refer to Dockerfile and docker-compose.yml.</p>"},{"location":"installation/#docker-with-docker-compose","title":"Docker with docker-compose","text":"<p>For the latest information, please check\u00a0docker-compose.yml.</p> <pre><code># run command\nexport TAG=v$(cat netspresso/VERSION) &amp;&amp; \\\ndocker compose run --service-ports --name netspresso-dev netspresso bash\n</code></pre>"},{"location":"installation/#docker-image-build","title":"Docker image build","text":"<p>If you run with\u00a0<code>docker run</code>\u00a0command, follow the image build and run command in the below:</p> <pre><code># build an image\ndocker build -t netspresso:v$(cat netspresso/VERSION) .\n</code></pre> <pre><code># docker run command\ndocker run -it --ipc=host\\\n  --gpus='\"device=0,1,2,3\"'\\  # your GPU id(s)\n  -v /PATH/TO/DATA:/DATA/PATH/IN/CONTAINER\\\n  -v /PATH/TO/CHECKPOINT:/CHECKPOINT/PATH/IN/CONTAINER\\\n  -p 50001:50001\\\n  -p 50002:50002\\\n  -p 50003:50003\\\n  -p 50004:50004\\\n  --name netspresso-dev netspresso:v$(cat netspresso/VERSION)\n</code></pre>"},{"location":"description/netspresso/compressor/automatic_compression/","title":"Automatic Compression","text":""},{"location":"description/netspresso/compressor/automatic_compression/#description","title":"Description","text":""},{"location":"description/netspresso/compressor/automatic_compression/#netspresso.compressor.v2.compressor.CompressorV2.automatic_compression","title":"<code>netspresso.compressor.v2.compressor.CompressorV2.automatic_compression(input_model_path, output_dir, input_shapes, framework=Framework.PYTORCH, compression_ratio=0.5)</code>","text":"<p>Compress a model automatically based on the given compression ratio.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local path to save the compressed model.</p> required <code>input_shapes</code> <code>List[Dict[str, int]]</code> <p>Input shapes of the model.</p> required <code>framework</code> <code>Framework</code> <p>The framework of the model.</p> <code>PYTORCH</code> <code>compression_ratio</code> <code>float</code> <p>The compression ratio for automatic compression. Defaults to 0.5.</p> <code>0.5</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs while performing automatic compression.</p> <p>Returns:</p> Name Type Description <code>CompressorMetadata</code> <code>CompressorMetadata</code> <p>Compress metadata.</p>"},{"location":"description/netspresso/compressor/automatic_compression/#details-of-parameters","title":"Details of Parameters","text":""},{"location":"description/netspresso/compressor/automatic_compression/#compression-ratio","title":"Compression Ratio","text":"<p>Note</p> <ul> <li>As the compression ratio increases, you can get more lighter and faster compressed models, but with greater lost of accuracy. </li> <li>Therefore, it is necessary to find an appropriate ratio for your requirements. It might require a few trials and errors.</li> <li>The range of available values is as follows.</li> </ul> \\[0 &lt; \\text{ratio} &lt; 1\\]"},{"location":"description/netspresso/compressor/automatic_compression/#example","title":"Example","text":"<pre><code>from netspresso import NetsPresso\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\ncompressor = netspresso.compressor_v2()\ncompressed_model = compressor.automatic_compression(\n    input_shapes=[{\"batch\": 1, \"channel\": 3, \"dimension\": [224, 224]}],\n    input_model_path=\"./examples/sample_models/graphmodule.pt\",\n    output_dir=\"./outputs/compressed/pytorch_automatic_compression_1\",\n    compression_ratio=0.5,\n)\n</code></pre>"},{"location":"description/netspresso/compressor/get_compression/","title":"Get Compression Information","text":"<p>.. autofunction:: netspresso.compressor.init.CompressorV2.get_compression</p>"},{"location":"description/netspresso/compressor/get_compression/#details-of-returns","title":"Details of Returns","text":"<p>.. autoclass:: netspresso.utils.metadata.default.compressor.CompressionInfo     :noindex:</p>"},{"location":"description/netspresso/compressor/get_compression/#example","title":"Example","text":"<p>.. code-block:: python</p> <pre><code>from netspresso import NetsPresso\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\ncompressor = netspresso.compressor_v2()\ncompressed_info = compressor.get_compression(compression_id=\"YOUR_COMPRESSION_ID\")\n</code></pre> <p>Output ~~~~~~</p> <p>.. code-block:: bash</p> <pre><code>&gt;&gt;&gt; compressed_info\nCompressionInfo(\n    compressed_model_id=\"c65ca574-08ab-4a42-9e82-a222bd089b2c\",\n    compression_id=\"YOUR_COMPRESSION_ID\",\n    compression_method=\"PR_L2\",\n    available_layers=[\n        AvailableLayer(name='conv1', values=[0.59375], channels=[32]), \n        AvailableLayer(name='layers.0.conv2', values=[0.25], channels=[64]), \n        AvailableLayer(name='layers.1.conv2', values=[0.25], channels=[128]), \n        AvailableLayer(name='layers.2.conv2', values=[0.34375], channels=[128]), \n        AvailableLayer(name='layers.3.conv2', values=[0.33203125], channels=[256]), \n        AvailableLayer(name='layers.4.conv2', values=[0.55859375], channels=[256]), \n        AvailableLayer(name='layers.5.conv2', values=[0.56640625], channels=[512]), \n        AvailableLayer(name='layers.6.conv2', values=[0.697265625], channels=[512]), \n        AvailableLayer(name='layers.7.conv2', values=[0.70703125], channels=[512]), \n        AvailableLayer(name='layers.8.conv2', values=[0.580078125], channels=[512]), \n        vailableLayer(name='layers.9.conv2', values=[0.51953125], channels=[512]), \n        AvailableLayer(name='layers.10.conv2', values=[0.517578125], channels=[512]), \n        AvailableLayer(name='layers.11.conv2', values=[0.7734375], channels=[1024]), \n        AvailableLayer(name='layers.12.conv2', values=[0.0234375], channels=[1024])\n    ],\n    options={'reshape_channel_axis': -1, 'policy': 'average', 'layer_norm': 'tss_norm', 'group_policy': 'average'}\n    original_model_id=\"\"\n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/","title":"Compression Method","text":""},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#structured-pruning","title":"Structured Pruning","text":""},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#pruning-by-criteria","title":"Pruning by Criteria","text":"<p>Difference of each pruning method is about measuring importance of filters in each layer. Filters in each layer will be automatically pruned based on certain criteria.</p>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#structured-neuron-level-pruning-snp","title":"Structured Neuron-level Pruning (SNP)","text":"<ul> <li>SNP prunes graphically connected query and key layers having the least informative attention scores while preserving the overall attention scores. Value layers, which can be pruned independently, are pruned to eliminate inter-head redundancy.</li> <li>Click the link for more information. (Structured Neuron-level Pruning)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#l2-norm-pruning","title":"L2 Norm Pruning","text":"<ul> <li>L2-Norm is used to represent the importance of the corresponding filter. In other words, this method prunes filters based on the magnitude of weights.</li> <li>Click the link for more information. (L2 Norm Pruning)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#gm-pruning","title":"GM Pruning","text":"<ul> <li>Geometric Median is used to measure the redundancy of the corresponding filter and remove redundant filters.</li> <li>Click the link for more information. (GM Pruning)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#nuclear-norm-pruning","title":"Nuclear Norm Pruning","text":"<ul> <li>The Nuclear Norm is the sum of the singular values representing the energy. It computes the nuclear norm on the feature map to determine the filter's relevance. For this reason, a portion of the dataset is needed.</li> <li>Click the link for more information. (Nuclear Norm Pruning)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#pruning-by-channel-index","title":"Pruning by Channel Index","text":"<ul> <li>This function prunes the chosen filters of each layer through the index without certain criteria.</li> <li>You can apply your own criteria to prune the model.</li> <li>If the selected filters are redundant or less important, it will return a better performing model.</li> <li>Click the link for more information. (Pruning By Index)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#filter-decomposition","title":"Filter Decomposition","text":""},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#tucker-decomposition","title":"Tucker Decomposition","text":"<ul> <li>Approximating the original filters by Tucker decomposition method.</li> <li>This method decomposes the convolution with a 4D kernel tensor into two factor matrices and one small core tensor.</li> <li>Click the link for more information. (Tucker Decomposition)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#singular-value-decomposition","title":"Singular Value Decomposition","text":"<ul> <li>Approximating the original filters by Singular value decomposition method.</li> <li>This method decomposes the pointwise convolution or fully-connected layer into two pointwise or fully-connected layers.</li> <li>Click the link for more information. (Singular Value Decomposition)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/compression_method/#cp-decomposition","title":"CP Decomposition","text":"<ul> <li>Approximating the original filters by CP decomposition method.</li> <li>This method approximates the convolution with a 4D kernel tensor by the sequence of four convolutions with small 2D kernel tensors.</li> <li>Click the link for more information. (CP Decomposition)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/","title":"Compressor(Manual Compression)","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#upload-model","title":"Upload Model","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#netspresso.compressor.v2.compressor.CompressorV2.upload_model","title":"<code>netspresso.compressor.v2.compressor.CompressorV2.upload_model(input_model_path, input_shapes=None, framework=Framework.PYTORCH)</code>","text":"<p>Upload a model for compression.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>input_shapes</code> <code>List[Dict[str, int]]</code> <p>Input shapes of the model. Defaults to [].</p> <code>None</code> <code>framework</code> <code>Framework</code> <p>The framework of the model.</p> <code>PYTORCH</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs while uploading the model.</p> <p>Returns:</p> Name Type Description <code>ModelBase</code> <code>ModelBase</code> <p>Uploaded model object.</p>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#details-of-parameters","title":"Details of Parameters","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#framework","title":"Framework","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#available-framework","title":"Available Framework","text":"Name Description TENSORFLOW_KERAS TensorFlow-Keras PYTORCH PyTorch GraphModule ONNX ONNX"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example","title":"Example","text":"<pre><code>from netspresso.enums import Framework\n\nFRAMEWORK = Framework.PYTORCH\n</code></pre> <p>Note</p> <ul> <li> <p>ONNX (.onnx)</p> <ul> <li>Supported version: PyTorch &gt;= 1.11.x, ONNX &gt;= 1.10.x.</li> <li>If a model is defined in PyTorch, it should be converted into the ONNX format before being uploaded.</li> <li>How-to-guide for the conversion of PyTorch into ONNX</li> </ul> </li> <li> <p>PyTorch GraphModule (.pt)</p> <ul> <li>Supported version: PyTorch &gt;= 1.11.x.</li> <li>If a model is defined in PyTorch, it should be converted into the GraphModule before being uploaded.</li> <li>The model must contain not only the status dictionary but also the structure of the model (do not use state_dict).</li> <li>How-to-guide for the conversion of PyTorch into GraphModule</li> </ul> </li> <li> <p>TensorFlow-Keras (.h5, .zip)</p> <ul> <li>Supported version: TensorFlow 2.3.x ~ 2.8.x.</li> <li>Custom layer must not be included in Keras H5 (.h5) format.</li> <li>The model must contain not only weights but also the structure of the model (do not use save_weights).</li> <li>If there is a custom layer in the model, please upload TensorFlow SavedModel format (.zip).</li> </ul> <p> </p> </li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#input-shapes","title":"Input Shapes","text":"<p>Note</p> <ul> <li> <p>For input shapes, use the same values that you used to train the model.</p> <ul> <li>If the input shapes of the model is dynamic, input shapes is required.</li> <li>If the input shapes of the model is static, input shapes is not required.</li> </ul> </li> <li> <p>For example, batch=1, channel=3, height=768, width=1024.</p> <pre><code>input_shapes = [{\"batch\": 1, \"channel\": 3, \"dimension\": [768, 1024]}]\n</code></pre> </li> <li> <p>Currently, only single input models are supported.</p> </li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example_1","title":"Example","text":"<pre><code>from netspresso import NetsPresso\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\ncompressor = netspresso.compressor_v2()\nmodel = compressor.upload_model(\n    input_model_path=\"./examples/sample_models/mobilenetv1.h5\",\n    input_shapes=[{\"batch\": 1, \"channel\": 3, \"dimension\": [224, 224]}],\n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#select-compression-method","title":"Select Compression Method","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#netspresso.compressor.v2.compressor.CompressorV2.select_compression_method","title":"<code>netspresso.compressor.v2.compressor.CompressorV2.select_compression_method(model_id, compression_method, options=Options())</code>","text":"<p>Select a compression method for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model.</p> required <code>compression_method</code> <code>CompressionMethod</code> <p>The selected compression method.</p> required <code>options(Options,</code> <code>optional</code> <p>The options for pruning method.</p> required <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs while selecting the compression method.</p> <p>Returns:</p> Name Type Description <code>ResponseSelectMethod</code> <code>ResponseSelectMethod</code> <p>The compression information for the selected compression method.</p>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#details-of-parameters_1","title":"Details of Parameters","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#compression-method","title":"Compression Method","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#available-compression-method","title":"Available Compression Method","text":"Name Description PR_L2 L2 Norm Pruning PR_GM GM Pruning PR_NN Nuclear Norm Pruning PR_SNP Structured Neuron-level Pruning PR_ID Pruning By Index FD_TK Tucker Decomposition FD_SVD Singular Value Decomposition FD_CP CP Decomposition"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example_2","title":"Example","text":"<pre><code>from netspresso.enums import CompressionMethod\n\nCOMPRESSION_METHOD = CompressionMethod.PR_L2\n</code></pre> <p>Warning</p> <ul> <li>Nuclear Norm is only supported in the Tensorflow-Keras framework.</li> <li>Structured Neuron-level is only supported in the PyTorch and ONNX frameworks.</li> </ul> <p>Note</p> <ul> <li>Click on the link to learn more about the information. (Compression Method)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#options","title":"Options","text":"<ul> <li><code>netspresso.enums.Policy</code> - Policy for handling connected filters</li> <li><code>netspresso.enums.LayerNorm</code> - Layer normalization method</li> <li><code>netspresso.enums.GroupPolicy</code> - Group policy for group convolutions</li> <li><code>netspresso.enums.StepOp</code> - Step operator for rounding</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example_3","title":"Example","text":"<pre><code>from netspresso.enums import Policy, LayerNorm, GroupPolicy\nfrom netspresso.clients.compressor.v2.schemas import Options\n\nOPTIONS = Options(\n    policy=Policy.AVERAGE,\n    layer_norm=LayerNorm.TSS_NORM,\n    group_policy=GroupPolicy.COUNT,\n    reshape_channel_axis=-1\n)\n</code></pre> <p>Note</p> <ul> <li>Click the link for more information. (Pruning Options)</li> </ul> <p>Note</p> <ul> <li>This parameter applies only to the Pruning Method (PR_L2, PR_GM, PR_NN, PR_SNP).</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#details-of-returns","title":"Details of Returns","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example_4","title":"Example","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import CompressionMethod, Policy, LayerNorm, GroupPolicy\nfrom netspresso.clients.compressor.v2.schemas import Options\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\ncompressor = netspresso.compressor_v2()\ncompression_info = compressor.select_compression_method(\n    model_id=\"YOUR_UPLOADED_MODEL_ID\",\n    compression_method=CompressionMethod.PR_L2,\n    options=Options(\n        policy=Policy.AVERAGE,\n        layer_norm=LayerNorm.STANDARD_SCORE,\n        group_policy=GroupPolicy.AVERAGE,\n        reshape_channel_axis=-1,\n    ),\n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#output","title":"Output","text":"<pre><code>&gt;&gt;&gt; compression_info\nCompressionInfo(\n    compression_method=\"PR_L2\", \n    available_layers=[\n        AvailableLayer(name='conv1', values=[\"\"], channels=[32]), \n        AvailableLayer(name='layers.0.conv2', values=[\"\"], channels=[64]), \n        AvailableLayer(name='layers.1.conv2', values=[\"\"], channels=[128]), \n        AvailableLayer(name='layers.2.conv2', values=[\"\"], channels=[128]), \n        AvailableLayer(name='layers.3.conv2', values=[\"\"], channels=[256]), \n        AvailableLayer(name='layers.4.conv2', values=[\"\"], channels=[256]), \n        AvailableLayer(name='layers.5.conv2', values=[\"\"], channels=[512]), \n        AvailableLayer(name='layers.6.conv2', values=[\"\"], channels=[512]), \n        AvailableLayer(name='layers.7.conv2', values=[\"\"], channels=[512]), \n        AvailableLayer(name='layers.8.conv2', values=[\"\"], channels=[512]), \n        AvailableLayer(name='layers.9.conv2', values=[\"\"], channels=[512]), \n        AvailableLayer(name='layers.10.conv2', values=[\"\"], channels=[512]), \n        AvailableLayer(name='layers.11.conv2', values=[\"\"], channels=[1024]), \n        AvailableLayer(name='layers.12.conv2', values=[\"\"], channels=[1024])\n    ], \n    options={'reshape_channel_axis': -1, 'policy': 'average', 'layer_norm': 'tss_norm', 'group_policy': 'average'}\n    original_model_id=\"YOUR_UPLOADED_MODEL_ID\",\n    compressed_model_id=\"\", \n    compression_id=\"\", \n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#set-compression-params","title":"Set Compression Params","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#details-of-parameters_2","title":"Details of Parameters","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#values-of-available-layer","title":"Values of available layer","text":"Compression Method Number of Values Type Range PR_L2 1 Float 0.0 &lt; ratio &lt; 1.0 PR_GM 1 Float 0.0 &lt; ratio &lt; 1.0 PR_NN 1 Float 0.0 &lt; ratio &lt; 1.0 PR_SNP 1 Float 0.0 &lt; ratio &lt; 1.0 PR_ID (Num of Out Channels - 1) Int 0 \u2264 channels &lt; Num of Out Channels FD_TK 2 Int 0 &lt; rank \u2264 (Num of In Channels or Num of Out Channels) FD_CP 1 Int 0 &lt; rank \u2264 min(Num of In Channels or Num of Out Channels) FD_SVD 1 Int 0 &lt; rank \u2264 min(Num of In Channels or Num of Out Channels)"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example_5","title":"Example","text":"<pre><code>for available_layer in compression_info.available_layers:\n   available_layer.values = [0.2]\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#output_1","title":"Output","text":"<pre><code>&gt;&gt;&gt; compression_info\nCompressionInfo(\n   compression_method=\"PR_L2\", \n   available_layers=[\n      AvailableLayer(name='conv1', values=[0.2], channels=[32]), \n      AvailableLayer(name='layers.0.conv2', values=[0.2], channels=[64]), \n      AvailableLayer(name='layers.1.conv2', values=[0.2], channels=[128]), \n      AvailableLayer(name='layers.2.conv2', values=[0.2], channels=[128]), \n      AvailableLayer(name='layers.3.conv2', values=[0.2], channels=[256]), \n      AvailableLayer(name='layers.4.conv2', values=[0.2], channels=[256]), \n      AvailableLayer(name='layers.5.conv2', values=[0.2], channels=[512]), \n      AvailableLayer(name='layers.6.conv2', values=[0.2], channels=[512]), \n      AvailableLayer(name='layers.7.conv2', values=[0.2], channels=[512]), \n      AvailableLayer(name='layers.8.conv2', values=[0.2], channels=[512]), \n      AvailableLayer(name='layers.9.conv2', values=[0.2], channels=[512]), \n      AvailableLayer(name='layers.10.conv2', values=[0.2], channels=[512]), \n      AvailableLayer(name='layers.11.conv2', values=[0.2], channels=[1024]), \n      AvailableLayer(name='layers.12.conv2', values=[0.2], channels=[1024])\n   ], \n   options={'reshape_channel_axis': -1, 'policy': 'average', 'layer_norm': 'tss_norm', 'group_policy': 'average'}\n   original_model_id=\"YOUR_UPLOADED_MODEL_ID\",\n   compressed_model_id=\"\", \n   compression_id=\"\", \n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#compress-model","title":"Compress Model","text":""},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#netspresso.compressor.v2.compressor.CompressorV2.compress_model","title":"<code>netspresso.compressor.v2.compressor.CompressorV2.compress_model(compression, output_dir, dataset_path=None)</code>","text":"<p>Compress a model using the provided compression information.</p> <p>Parameters:</p> Name Type Description Default <code>compression</code> <code>CompressionInfo</code> <p>The information about the compression.</p> required <code>output_dir</code> <code>str</code> <p>The local path to save the compressed model.</p> required <code>dataset_path</code> <code>str</code> <p>The path of the dataset used for nuclear norm compression method. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs while compressing the model.</p> <p>Returns:</p> Name Type Description <code>CompressorMetadata</code> <code>CompressorMetadata</code> <p>Compress metadata.</p>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#example_6","title":"Example","text":"<pre><code>compressed_model = compressor.compress_model(\n    compression=compression_info,\n    output_dir=\"./outputs/compressed/graphmodule_manual\",\n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/manual_compression/#full-example","title":"Full Example","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import CompressionMethod, GroupPolicy, LayerNorm, Policy\nfrom netspresso.clients.compressor.v2.schemas import Options\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\n# 1. Declare compressor\ncompressor = netspresso.compressor_v2()\n\n# 2. Upload model\nmodel = compressor.upload_model(\n    input_model_path=\"./examples/sample_models/graphmodule.pt\",\n    input_shapes=[{\"batch\": 1, \"channel\": 3, \"dimension\": [224, 224]}],\n)\n\n# 3. Select compression method\ncompression_info = compressor.select_compression_method(\n    model_id=model.ai_model_id,\n    compression_method=CompressionMethod.PR_L2,\n    options=Options(\n        policy=Policy.AVERAGE,\n        layer_norm=LayerNorm.STANDARD_SCORE,\n        group_policy=GroupPolicy.AVERAGE,\n        reshape_channel_axis=-1,\n    ),\n)\n\n# 4. Set params for compression(ratio or rank)\nfor available_layer in compression_info.available_layers[:5]:\n    available_layer.values = [0.2]\n\n# 5. Compress model\ncompressed_model = compressor.compress_model(\n    compression=compression_info,\n    output_dir=\"./outputs/compressed/graphmodule_manual\",\n)\n</code></pre>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/","title":"Pruning Options","text":""},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#policy","title":"Policy","text":"<p>The policy of the NetsPresso Model Compressor allows for pruning connected filters or neurons identically using the given pruning criteria while preserving the information.</p> <p>To calculate the importance score of the connected neurons or filters, \"Sum\", \"Average\" can be used as policies (for more details, please refer to the following documents).</p>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#sum","title":"Sum","text":"<ul> <li>The \"Sum\" policy calculates its importance score as the summation value of the connected filters' importance scores.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#average","title":"Average","text":"<ul> <li>The \"Average\" policy calculates its importance score as the average value of the connected filters' importance scores.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#layer-norm","title":"Layer Norm","text":"<p>The normalization process is necessary to compare the importance score of different layer's filters or neurons.</p>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#standard-score","title":"Standard Score","text":""},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#tss-norm-total-sum-scaling-normalization","title":"TSS Norm (Total sum scaling normalization)","text":""},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#linear-scaling","title":"Linear Scaling","text":""},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#softmax-norm","title":"Softmax Norm","text":""},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#none","title":"None","text":"<ul> <li>Normalization will not be used to compare the importance of the different layer.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#group-policy","title":"Group Policy","text":"<p>The reshape and group convolutional operator should prune the same number of filters for each group to preserve the shape of the weight or arguments.</p> <p>For this reason, the group policy is used to ensure that the same number of filters are pruned for each group.</p>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#sum_1","title":"Sum","text":"<ul> <li>The group policy \"sum\" calculates its importance score as the summation value of the corresponding filter index of all groups.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#average_1","title":"Average","text":"<ul> <li>The group policy \"Average\" calculates its importance score as the average value of the corresponding filter index of all groups.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#count","title":"Count","text":"<ul> <li>The importance score of each groups' filter will be measured independently, and the minimum number of filters for the given group will be removed identically for given groups.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#none_1","title":"None","text":"<ul> <li>The group policy \"None\" will prune same amount of the filters to preserve the shape of the weight. None of the policy will be used to represent the filter index of group.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#reshape-channel-axis","title":"Reshape Channel Axis","text":"<p>Reshape channel axis represents which axis of the reshape operator will be pruned.</p> <ul> <li> <p>Ex. Consider the input of the given reshape operator is <code>batch, 768, 197</code> and the output of the reshape operator is <code>batch, 12, 64, 197</code></p> <ul> <li> <p>If the <code>reshape_channel_axis</code> is <code>-1</code> or <code>1</code> when the given pruning ratio is 50%, the output model will contain <code>32 (64*0.5)</code> channels of the given reshape operator.</p> </li> <li> <p>If the <code>reshape_channel_axis</code> is <code>-2</code> or <code>0</code> when the given pruning ratio is 50%, the output model will contain <code>6 (12*0.5)</code> channels of the given reshape operator.</p> </li> </ul> </li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#step-operator","title":"Step operator","text":"<p>Step operator is the method of rounding applied to ensure that the amount remaining after pruning aligns with the step_size.</p> <p>Options include Round, Round Up, Round Down, or None.</p>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#round","title":"Round","text":"<ul> <li>Rounds to the nearest step size, adjusting the remaining count of filters.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#round-up","title":"Round Up","text":"<ul> <li>Always rounds up to the next step size, directly affecting the remaining filters.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#round-down","title":"Round Down","text":"<ul> <li>Always rounds down to the closest lower step size, impacting the remaining filters.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/pruning_options/#none_2","title":"None","text":"<ul> <li>No rounding operation is applied; the exact amount is used for pruning.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/","title":"Compressor(Recommendation Compression)","text":""},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#description","title":"Description","text":""},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#netspresso.compressor.v2.compressor.CompressorV2.recommendation_compression","title":"<code>netspresso.compressor.v2.compressor.CompressorV2.recommendation_compression(compression_method, recommendation_method, recommendation_ratio, input_model_path, output_dir, input_shapes, framework=Framework.PYTORCH, options=RecommendationOptions(), dataset_path=None)</code>","text":"<p>Compress a recommendation-based model using the given compression and recommendation methods.</p> <p>Parameters:</p> Name Type Description Default <code>compression_method</code> <code>CompressionMethod</code> <p>The selected compression method.</p> required <code>recommendation_method</code> <code>RecommendationMethod</code> <p>The selected recommendation method.</p> required <code>recommendation_ratio</code> <code>float</code> <p>The compression ratio recommended by the recommendation method.</p> required <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local path to save the compressed model.</p> required <code>input_shapes</code> <code>List[Dict[str, int]]</code> <p>Input shapes of the model.</p> required <code>framework</code> <code>Framework</code> <p>The framework of the model.</p> <code>PYTORCH</code> <code>options(Options,</code> <code>optional</code> <p>The options for pruning method.</p> required <code>dataset_path</code> <code>str</code> <p>The path of the dataset used for nuclear norm compression method. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs while performing recommendation compression.</p> <p>Returns:</p> Name Type Description <code>CompressorMetadata</code> <code>CompressorMetadata</code> <p>Compress metadata.</p>"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#details-of-parameters","title":"Details of Parameters","text":""},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#compression-method","title":"Compression Method","text":""},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#available-compression-method","title":"Available Compression Method","text":"Name Description PR_L2 L2 Norm Pruning PR_GM GM Pruning PR_NN Nuclear Norm Pruning PR_SNP Structured Neuron-level Pruning FD_TK Tucker Decomposition FD_SVD Singular Value Decomposition"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#example","title":"Example","text":"<pre><code>from netspresso.enums import CompressionMethod\n\nCOMPRESSION_METHOD = CompressionMethod.PR_L2\n</code></pre> <p>Warning</p> <ul> <li>Nuclear Norm is only supported in the Tensorflow-Keras framework.</li> <li>Structured Neuron-level is only supported in the PyTorch and ONNX frameworks.</li> </ul> <p>Note</p> <ul> <li>Click on the link to learn more about the information. (Compression Method)</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#recommendation-method","title":"Recommendation Method","text":""},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#available-recommendation-method","title":"Available Recommendation Method","text":"Name Description SLAMP Structured Layer-adaptive Sparsity for the Magnitude-based Pruning VBMF Variational Bayesian Matrix Factorization"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#example_1","title":"Example","text":"<pre><code>from netspresso.enums import RecommendationMethod\n\nRECOMMENDATION_METHOD = RecommendationMethod.SLAMP\n</code></pre> <p>Note</p> <ul> <li>If you selected PR_L2, PR_GM, PR_NN, PR_SNP for compression_method<ul> <li>The recommended_method available is SLAMP.</li> </ul> </li> <li>If you selected FD_TK, FD_SVD for compression_method<ul> <li>The recommended_method available is VBMF.</li> </ul> </li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#recommendation-ratio","title":"Recommendation Ratio","text":"<ul> <li> <p>SLAMP (Pruning ratio)</p> <ul> <li>Remove corresponding amounts of the filters. (e.g. 0.2 removes 20% of the filters in each layer)</li> <li> <p>Available ranges: <code>0 &lt; ratio &lt; 1</code></p> </li> <li> <p>Click the link for more information. (SLAMP)</p> </li> </ul> </li> <li> <p>VBMF (Calibration ratio)</p> <ul> <li>This function control compression level of model if the result of recommendation doesn't meet the compression level user wants. Remained rank add or subtract (removed rank x calibration ratio) according to calibration ratio range.  </li> <li> <p>Available ranges: <code>-1 \u2264 ratio \u2264 1</code></p> </li> <li> <p>Click the link for more information. (VBMF)</p> </li> </ul> </li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#options","title":"Options","text":""},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#example_2","title":"Example","text":"<pre><code>from netspresso.enums import Policy, LayerNorm, GroupPolicy\nfrom netspresso.clients.compressor.v2.schemas import Options\n\nOPTIONS = Options(\n    policy=Policy.AVERAGE,\n    layer_norm=LayerNorm.TSS_NORM,\n    group_policy=GroupPolicy.COUNT,\n    reshape_channel_axis=-1\n)\n</code></pre> <p>Note</p> <ul> <li>Click the link for more information. (Pruning Options)</li> </ul> <p>Warning</p> <ul> <li>Nuclear Norm is only supported in the Tensorflow-Keras framework.</li> <li>Structured Neuron-level is only supported in the PyTorch and ONNX frameworks.</li> </ul>"},{"location":"description/netspresso/compressor/advanced_compression/recommendation_compression/#example_3","title":"Example","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import CompressionMethod, RecommendationMethod\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\ncompressor = netspresso.compressor_v2()\ncompressed_model = compressor.recommendation_compression(\n    compression_method=CompressionMethod.PR_L2,\n    recommendation_method=RecommendationMethod.SLAMP,\n    recommendation_ratio=0.5,\n    input_model_path=\"./examples/sample_models/graphmodule.pt\",\n    output_dir=\"./outputs/compressed/graphmodule_recommend\",\n    input_shapes=[{\"batch\": 1, \"channel\": 3, \"dimension\": [224, 224]}],\n)\n</code></pre>"},{"location":"description/netspresso/converter/converter/","title":"Trainer","text":""},{"location":"description/netspresso/converter/converter/#description","title":"Description","text":""},{"location":"description/netspresso/converter/converter/#netspresso.converter.v2.converter.ConverterV2","title":"<code>netspresso.converter.v2.converter.ConverterV2</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/converter/converter/#netspresso.converter.v2.converter.ConverterV2.__init__","title":"<code>__init__(token_handler, user_info)</code>","text":"<p>Initialize the Converter.</p>"},{"location":"description/netspresso/converter/converter/#netspresso.converter.v2.converter.ConverterV2.cancel_conversion_task","title":"<code>cancel_conversion_task(conversion_task_id)</code>","text":"<p>Cancel the conversion task with given conversion task uuid.</p> <p>Parameters:</p> Name Type Description Default <code>conversion_task_id</code> <code>str</code> <p>Convert task UUID of the convert task.</p> required <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the task cancel.</p> <p>Returns:</p> Name Type Description <code>ConversionTask</code> <code>ConvertTask</code> <p>Model conversion task dictionary.</p>"},{"location":"description/netspresso/converter/converter/#netspresso.converter.v2.converter.ConverterV2.convert_model","title":"<code>convert_model(input_model_path, output_dir, target_framework, target_device_name, target_data_type=DataType.FP16, target_software_version=None, input_layer=None, dataset_path=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Convert a model to the specified framework.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local folder path to save the converted model.</p> required <code>target_framework</code> <code>Union[str, Framework]</code> <p>The target framework name.</p> required <code>target_device_name</code> <code>Union[str, DeviceName]</code> <p>Target device name. Required if target_device is not specified.</p> required <code>target_data_type</code> <code>Union[str, DataType]</code> <p>Data type of the model. Default is DataType.FP16.</p> <code>FP16</code> <code>target_software_version</code> <code>Union[str, SoftwareVersion]</code> <p>Target software version. Required if target_device_name is one of the Jetson devices.</p> <code>None</code> <code>input_layer</code> <code>InputShape</code> <p>Target input shape for conversion (e.g., dynamic batch to static batch).</p> <code>None</code> <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Useful for certain conversions.</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, wait for the conversion result before returning the function.                 If False, request the conversion and return  the function immediately.</p> <code>True</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model conversion.</p> <p>Returns:</p> Name Type Description <code>ConverterMetadata</code> <code>ConverterMetadata</code> <p>Convert metadata.</p>"},{"location":"description/netspresso/converter/converter/#netspresso.converter.v2.converter.ConverterV2.get_conversion_task","title":"<code>get_conversion_task(conversion_task_id)</code>","text":"<p>Get the conversion task information with given conversion task uuid.</p> <p>Parameters:</p> Name Type Description Default <code>conversion_task_id</code> <code>str</code> <p>Convert task UUID of the convert task.</p> required <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model conversion.</p> <p>Returns:</p> Name Type Description <code>ConversionTask</code> <code>ConvertTask</code> <p>Model conversion task dictionary.</p>"},{"location":"description/netspresso/converter/converter/#examples","title":"Examples","text":""},{"location":"description/netspresso/converter/converter/#converting-a-model-to-tensorrt-on-jetson-agx-orin","title":"Converting a model to TensorRT on Jetson AGX Orin","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import DeviceName, Framework, SoftwareVersion\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nconverter = netspresso.converter_v2()\nconversion_task = converter.convert_model(\n   input_model_path=\"./examples/sample_models/test.onnx\",\n   output_dir=\"./outputs/converted/TENSORRT_JETSON_AGX_ORIN_JETPACK_5_0_1\",\n   target_framework=Framework.TENSORRT,\n   target_device_name=DeviceName.JETSON_AGX_ORIN,\n   target_software_version=SoftwareVersion.JETPACK_5_0_1,\n)\n</code></pre>"},{"location":"description/netspresso/enums/compression_method/","title":"Compression Method","text":""},{"location":"description/netspresso/enums/compression_method/#netspresso.enums.CompressionMethod","title":"<code>netspresso.enums.CompressionMethod</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/compression_method/#available-compression-methods","title":"Available Compression Methods","text":"Name Value Description Type PR_L2 PR_L2 L2 Norm Pruning Pruning PR_GM PR_GM GM Pruning Pruning PR_NN PR_NN Nuclear Norm Pruning Pruning PR_SNP PR_SNP Structured Neuron-level Pruning Pruning PR_ID PR_ID Pruning By Index Pruning FD_TK FD_TK Tucker Decomposition Factorization FD_SVD FD_SVD Singular Value Decomposition Factorization FD_CP FD_CP CP Decomposition Factorization"},{"location":"description/netspresso/enums/compression_method/#framework-compatibility","title":"Framework Compatibility","text":"<p>Warning</p> <ul> <li>Nuclear Norm (PR_NN) is only supported in the Tensorflow-Keras framework.</li> <li>Structured Neuron-level (PR_SNP) is only supported in the PyTorch and ONNX frameworks.</li> <li>All other methods are supported across all frameworks.</li> </ul>"},{"location":"description/netspresso/enums/compression_method/#example","title":"Example","text":"<pre><code>from netspresso.enums import CompressionMethod\n\n# Select a compression method\ncompression_method = CompressionMethod.PR_L2\n\n# Available methods\nprint(f\"L2 Norm Pruning: {CompressionMethod.PR_L2}\")\nprint(f\"GM Pruning: {CompressionMethod.PR_GM}\")\nprint(f\"Tucker Decomposition: {CompressionMethod.FD_TK}\")\nprint(f\"CP Decomposition: {CompressionMethod.FD_CP}\")\n</code></pre>"},{"location":"description/netspresso/enums/data_type/","title":"DataType","text":""},{"location":"description/netspresso/enums/data_type/#netspresso.enums.DataType","title":"<code>netspresso.enums.DataType</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/data_type/#available-data-types","title":"Available Data Types","text":"Name Value Description Bits FP32 FP32 32-bit floating point 32 FP16 FP16 16-bit floating point 16 INT8 INT8 8-bit signed integer 8 NONE \"\" No data type specified -"},{"location":"description/netspresso/enums/data_type/#example","title":"Example","text":"<pre><code>from netspresso.enums import DataType\n\n# Select a data type\ndata_type = DataType.FP32\n\n# Available data types\nprint(f\"FP32: {DataType.FP32}\")\nprint(f\"FP16: {DataType.FP16}\")\nprint(f\"INT8: {DataType.INT8}\")\nprint(f\"None: {DataType.NONE}\")\n</code></pre>"},{"location":"description/netspresso/enums/device_name/","title":"DeviceName","text":""},{"location":"description/netspresso/enums/device_name/#netspresso.enums.DeviceName","title":"<code>netspresso.enums.DeviceName</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/device_name/#available-device-names","title":"Available Device Names","text":"Name Value Description Category RASPBERRY_PI_5 RaspberryPi5 Raspberry Pi 5 Raspberry Pi RASPBERRY_PI_4B RaspberryPi4B Raspberry Pi 4 Model B Raspberry Pi RASPBERRY_PI_3B_PLUS RaspberryPi3BPlus Raspberry Pi 3 Model B+ Raspberry Pi RASPBERRY_PI_3B RaspberryPi3B Raspberry Pi 3 Model B Raspberry Pi RASPBERRY_PI_2B RaspberryPi2B Raspberry Pi 2 Model B Raspberry Pi RASPBERRY_PI_ZERO_W RaspberryPi-ZeroW Raspberry Pi Zero W Raspberry Pi RASPBERRY_PI_ZERO_2W RaspberryPi-Zero2W Raspberry Pi Zero 2 W Raspberry Pi RENESAS_RZ_V2L rzv2l_avnet Renesas RZ/V2L Renesas RENESAS_RZ_V2M rzv2m Renesas RZ/V2M Renesas RENESAS_RA8D1 Renesas-RA8D1 Renesas RA8D1 Renesas JETSON_NANO Jetson-Nano NVIDIA Jetson Nano Jetson JETSON_TX2 Jetson-Tx2 NVIDIA Jetson TX2 Jetson JETSON_XAVIER Jetson-Xavier NVIDIA Jetson Xavier Jetson JETSON_NX Jetson-Nx NVIDIA Jetson NX Jetson JETSON_AGX_ORIN Jetson-AGX-Orin NVIDIA Jetson AGX Orin Jetson JETSON_ORIN_NANO Jetson-Orin-Nano NVIDIA Jetson Orin Nano Jetson AWS_T4 AWS-T4 AWS T4 Instance Cloud INTEL_XEON_W_2233 Intel-Xeon Intel Xeon W-2233 Intel ALIF_ENSEMBLE_E7_DEVKIT_GEN2 Ensemble-E7-DevKit-Gen2 Alif Ensemble E7 DevKit Gen2 Alif ARM_ETHOS_U_SERIES Arm Virtual Hardware Ethos-U Series ARM Ethos-U Series ARM NXP_iMX93 nxp_imx93_ethos_u65 NXP iMX93 with Ethos-U65 NXP ARDUINO_NICLA_VISION arduino_nicla_vision Arduino Nicla Vision Arduino"},{"location":"description/netspresso/enums/device_name/#example","title":"Example","text":"<pre><code>from netspresso.enums import DeviceName\n\n# Select a device\ndevice = DeviceName.JETSON_NANO\n\n# Available devices\nprint(f\"Raspberry Pi 5: {DeviceName.RASPBERRY_PI_5}\")\nprint(f\"Jetson Nano: {DeviceName.JETSON_NANO}\")\nprint(f\"AWS T4: {DeviceName.AWS_T4}\")\nprint(f\"Intel Xeon: {DeviceName.INTEL_XEON_W_2233}\")\n</code></pre>"},{"location":"description/netspresso/enums/extension/","title":"Extension","text":""},{"location":"description/netspresso/enums/extension/#netspresso.enums.Extension","title":"<code>netspresso.enums.Extension</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/extension/#available-file-extensions","title":"Available File Extensions","text":"Name Value Description Framework H5 h5 Keras HDF5 model file TensorFlow ZIP zip TensorFlow SavedModel archive TensorFlow PT pt PyTorch model file PyTorch ONNX onnx ONNX model file ONNX"},{"location":"description/netspresso/enums/extension/#example","title":"Example","text":"<pre><code>from netspresso.enums import Extension\n\n# Select an extension\nextension = Extension.ONNX\n\n# Available extensions\nprint(f\"H5: {Extension.H5}\")\nprint(f\"ZIP: {Extension.ZIP}\")\nprint(f\"PT: {Extension.PT}\")\nprint(f\"ONNX: {Extension.ONNX}\")\n</code></pre>"},{"location":"description/netspresso/enums/framework/","title":"Framework","text":""},{"location":"description/netspresso/enums/framework/#netspresso.enums.Framework","title":"<code>netspresso.enums.Framework</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/framework/#available-frameworks","title":"Available Frameworks","text":"Name Value Description File Extension TENSORFLOW_KERAS tensorflow_keras TensorFlow-Keras .h5, .zip TENSORFLOW saved_model TensorFlow SavedModel .zip PYTORCH pytorch PyTorch .pt ONNX onnx ONNX .onnx TENSORRT tensorrt TensorRT .engine OPENVINO openvino OpenVINO .xml TENSORFLOW_LITE tensorflow_lite TensorFlow Lite .tflite DRPAI drpai DRP-AI .onnx"},{"location":"description/netspresso/enums/framework/#framework-support","title":"Framework Support","text":"<p>Note</p> <ul> <li>Compressor Supported: TENSORFLOW_KERAS, PYTORCH, ONNX</li> <li>Launcher Supported: ONNX, TENSORRT, OPENVINO, TENSORFLOW_LITE, DRPAI, KERAS, SAVED_MODEL</li> <li>ONNX (.onnx): Supported version PyTorch &gt;= 1.11.x, ONNX &gt;= 1.10.x</li> <li>PyTorch (.pt): Supported version PyTorch &gt;= 1.11.x</li> <li>TensorFlow-Keras (.h5, .zip): Supported version TensorFlow 2.3.x ~ 2.8.x</li> </ul>"},{"location":"description/netspresso/enums/framework/#example","title":"Example","text":"<pre><code>from netspresso.enums import Framework\n\n# Select a framework\nframework = Framework.PYTORCH\n\n# Available frameworks\nprint(f\"TensorFlow-Keras: {Framework.TENSORFLOW_KERAS}\")\nprint(f\"PyTorch: {Framework.PYTORCH}\")\nprint(f\"ONNX: {Framework.ONNX}\")\nprint(f\"TensorRT: {Framework.TENSORRT}\")\n</code></pre>"},{"location":"description/netspresso/enums/group_policy/","title":"GroupPolicy","text":""},{"location":"description/netspresso/enums/group_policy/#netspresso.enums.GroupPolicy","title":"<code>netspresso.enums.GroupPolicy</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/group_policy/#available-group-policies","title":"Available Group Policies","text":"Name Value Description SUM sum Sum group policy for group convolutions AVERAGE average Average group policy for group convolutions COUNT count Count group policy for group convolutions NONE none No group policy applied"},{"location":"description/netspresso/enums/group_policy/#example","title":"Example","text":"<pre><code>from netspresso.enums import GroupPolicy\n\n# Select a group policy\ngroup_policy = GroupPolicy.AVERAGE\n\n# Available policies\nprint(f\"Sum: {GroupPolicy.SUM}\")\nprint(f\"Average: {GroupPolicy.AVERAGE}\")\nprint(f\"Count: {GroupPolicy.COUNT}\")\nprint(f\"None: {GroupPolicy.NONE}\")\n</code></pre>"},{"location":"description/netspresso/enums/hardware_type/","title":"HardwareType","text":""},{"location":"description/netspresso/enums/hardware_type/#netspresso.enums.HardwareType","title":"<code>netspresso.enums.HardwareType</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/hardware_type/#available-hardware-types","title":"Available Hardware Types","text":"Name Value Description Category HELIUM helium ARM Helium vector processing unit ARM"},{"location":"description/netspresso/enums/hardware_type/#example","title":"Example","text":"<pre><code>from netspresso.enums import HardwareType\n\n# Select a hardware type\nhardware = HardwareType.HELIUM\n\n# Available hardware types\nprint(f\"Helium: {HardwareType.HELIUM}\")\n</code></pre>"},{"location":"description/netspresso/enums/layer_norm/","title":"LayerNorm","text":""},{"location":"description/netspresso/enums/layer_norm/#netspresso.enums.LayerNorm","title":"<code>netspresso.enums.LayerNorm</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/layer_norm/#available-layer-normalization-methods","title":"Available Layer Normalization Methods","text":"Name Value Description NONE none No layer normalization STANDARD_SCORE standard_score Standard score normalization TSS_NORM tss_norm TSS normalization method LINEAR_SCALING linear_scaling Linear scaling normalization SOFTMAX_NORM softmax_norm Softmax normalization method"},{"location":"description/netspresso/enums/layer_norm/#example","title":"Example","text":"<pre><code>from netspresso.enums import LayerNorm\n\n# Select a layer normalization method\nlayer_norm = LayerNorm.STANDARD_SCORE\n\n# Available methods\nprint(f\"None: {LayerNorm.NONE}\")\nprint(f\"Standard Score: {LayerNorm.STANDARD_SCORE}\")\nprint(f\"TSS Norm: {LayerNorm.TSS_NORM}\")\nprint(f\"Linear Scaling: {LayerNorm.LINEAR_SCALING}\")\nprint(f\"Softmax Norm: {LayerNorm.SOFTMAX_NORM}\")\n</code></pre>"},{"location":"description/netspresso/enums/origin_from/","title":"OriginFrom","text":""},{"location":"description/netspresso/enums/origin_from/#netspresso.enums.OriginFrom","title":"<code>netspresso.enums.OriginFrom</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/origin_from/#available-origin-sources","title":"Available Origin Sources","text":"Name Value Description Source CUSTOM custom Custom user-uploaded model Custom NPMS npms NetsPresso Model Store model NPMS"},{"location":"description/netspresso/enums/origin_from/#example","title":"Example","text":"<pre><code>from netspresso.enums import OriginFrom\n\n# Select an origin source\norigin = OriginFrom.CUSTOM\n\n# Available origins\nprint(f\"Custom: {OriginFrom.CUSTOM}\")\nprint(f\"NPMS: {OriginFrom.NPMS}\")\n</code></pre>"},{"location":"description/netspresso/enums/policy/","title":"Policy","text":""},{"location":"description/netspresso/enums/policy/#netspresso.enums.Policy","title":"<code>netspresso.enums.Policy</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/policy/#available-policies","title":"Available Policies","text":"Name Value Description SUM sum Sum policy for handling connected filters AVERAGE average Average policy for handling connected filters"},{"location":"description/netspresso/enums/policy/#example","title":"Example","text":"<pre><code>from netspresso.enums import Policy\n\n# Select a policy\npolicy = Policy.AVERAGE\n\n# Available policies\nprint(f\"Sum: {Policy.SUM}\")\nprint(f\"Average: {Policy.AVERAGE}\")\n</code></pre>"},{"location":"description/netspresso/enums/quantization_mode/","title":"QuantizationMode","text":""},{"location":"description/netspresso/enums/quantization_mode/#netspresso.enums.QuantizationMode","title":"<code>netspresso.enums.QuantizationMode</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/quantization_mode/#available-quantization-modes","title":"Available Quantization Modes","text":"Name Value Description AUTOMATIC_QUANTIZATION automatic_quantization Automatic quantization mode UNIFORM_PRECISION_QUANTIZATION uniform_precision_quantization Uniform precision quantization mode CUSTOM_PRECISION_QUANTIZATION custom_precision_quantization Custom precision quantization mode ADVANCED_QUANTIZATION advanced_quantization Advanced quantization mode RECOMMEND_QUANTIZATION recommend_quantization Recommend quantization mode"},{"location":"description/netspresso/enums/quantization_mode/#example","title":"Example","text":"<pre><code>from netspresso.enums import QuantizationMode\n\n# Select a quantization mode\nmode = QuantizationMode.AUTOMATIC_QUANTIZATION\n\n# Available modes\nprint(f\"Automatic: {QuantizationMode.AUTOMATIC_QUANTIZATION}\")\nprint(f\"Uniform Precision: {QuantizationMode.UNIFORM_PRECISION_QUANTIZATION}\")\nprint(f\"Custom Precision: {QuantizationMode.CUSTOM_PRECISION_QUANTIZATION}\")\nprint(f\"Advanced: {QuantizationMode.ADVANCED_QUANTIZATION}\")\nprint(f\"Recommend: {QuantizationMode.RECOMMEND_QUANTIZATION}\")\n</code></pre>"},{"location":"description/netspresso/enums/quantization_precision/","title":"QuantizationPrecision","text":""},{"location":"description/netspresso/enums/quantization_precision/#netspresso.enums.QuantizationPrecision","title":"<code>netspresso.enums.QuantizationPrecision</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/quantization_precision/#available-quantization-precisions","title":"Available Quantization Precisions","text":"Name Value Description Bits INT8 int8 8-bit integer precision 8 FLOAT16 float16 16-bit floating precision 16 FLOAT32 float32 32-bit floating precision 32"},{"location":"description/netspresso/enums/quantization_precision/#example","title":"Example","text":"<pre><code>from netspresso.enums import QuantizationPrecision\n\n# Select a quantization precision\nprecision = QuantizationPrecision.INT8\n\n# Available precisions\nprint(f\"INT8: {QuantizationPrecision.INT8}\")\nprint(f\"FLOAT16: {QuantizationPrecision.FLOAT16}\")\nprint(f\"FLOAT32: {QuantizationPrecision.FLOAT32}\")\n</code></pre>"},{"location":"description/netspresso/enums/recommendation_method/","title":"RecommendationMethod","text":""},{"location":"description/netspresso/enums/recommendation_method/#netspresso.enums.RecommendationMethod","title":"<code>netspresso.enums.RecommendationMethod</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/recommendation_method/#available-recommendation-methods","title":"Available Recommendation Methods","text":"Name Value Description Type SLAMP slamp SLAMP recommendation method Structured VBMF vbmf Variational Bayesian Matrix Factorization Factorization"},{"location":"description/netspresso/enums/recommendation_method/#example","title":"Example","text":"<pre><code>from netspresso.enums import RecommendationMethod\n\n# Select a recommendation method\nmethod = RecommendationMethod.SLAMP\n\n# Available methods\nprint(f\"SLAMP: {RecommendationMethod.SLAMP}\")\nprint(f\"VBMF: {RecommendationMethod.VBMF}\")\n</code></pre>"},{"location":"description/netspresso/enums/similarity_metric/","title":"SimilarityMetric","text":""},{"location":"description/netspresso/enums/similarity_metric/#netspresso.enums.SimilarityMetric","title":"<code>netspresso.enums.SimilarityMetric</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/similarity_metric/#available-similarity-metrics","title":"Available Similarity Metrics","text":"Name Value Description Type SNR SNR Signal-to-Noise Ratio metric Quality"},{"location":"description/netspresso/enums/similarity_metric/#example","title":"Example","text":"<pre><code>from netspresso.enums import SimilarityMetric\n\n# Select a similarity metric\nmetric = SimilarityMetric.SNR\n\n# Available metrics\nprint(f\"SNR: {SimilarityMetric.SNR}\")\n</code></pre>"},{"location":"description/netspresso/enums/software_version/","title":"SoftwareVersion","text":""},{"location":"description/netspresso/enums/software_version/#netspresso.enums.SoftwareVersion","title":"<code>netspresso.enums.SoftwareVersion</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/software_version/#available-software-versions","title":"Available Software Versions","text":"Name Value Description JetPack Version JETPACK_4_4_1 4.4.1-b50 JetPack 4.4.1 4.4.1 JETPACK_4_6 4.6-b199 JetPack 4.6 4.6 JETPACK_5_0_1 5.0.1-b118 JetPack 5.0.1 5.0.1 JETPACK_5_0_2 5.0.2-b231 JetPack 5.0.2 5.0.2 JETPACK_6_1 6.1+b123 JetPack 6.1 6.1"},{"location":"description/netspresso/enums/software_version/#example","title":"Example","text":"<pre><code>from netspresso.enums import SoftwareVersion\n\n# Select a software version\nversion = SoftwareVersion.JETPACK_5_0_2\n\n# Available versions\nprint(f\"JetPack 4.4.1: {SoftwareVersion.JETPACK_4_4_1}\")\nprint(f\"JetPack 4.6: {SoftwareVersion.JETPACK_4_6}\")\nprint(f\"JetPack 5.0.1: {SoftwareVersion.JETPACK_5_0_1}\")\nprint(f\"JetPack 5.0.2: {SoftwareVersion.JETPACK_5_0_2}\")\nprint(f\"JetPack 6.1: {SoftwareVersion.JETPACK_6_1}\")\n</code></pre>"},{"location":"description/netspresso/enums/task/","title":"Task","text":""},{"location":"description/netspresso/enums/task/#netspresso.enums.Task","title":"<code>netspresso.enums.Task</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/task/#available-tasks","title":"Available Tasks","text":"Name Value Description Domain IMAGE_CLASSIFICATION classification Image classification task Computer Vision OBJECT_DETECTION detection Object detection task Computer Vision SEMANTIC_SEGMENTATION segmentation Semantic segmentation task Computer Vision"},{"location":"description/netspresso/enums/task/#example","title":"Example","text":"<pre><code>from netspresso.enums import Task\n\n# Select a task\ntask = Task.IMAGE_CLASSIFICATION\n\n# Available tasks\nprint(f\"Image Classification: {Task.IMAGE_CLASSIFICATION}\")\nprint(f\"Object Detection: {Task.OBJECT_DETECTION}\")\nprint(f\"Semantic Segmentation: {Task.SEMANTIC_SEGMENTATION}\")\n</code></pre>"},{"location":"description/netspresso/enums/task_status/","title":"TaskStatus","text":""},{"location":"description/netspresso/enums/task_status/#netspresso.enums.TaskStatus","title":"<code>netspresso.enums.TaskStatus</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"description/netspresso/enums/task_status/#available-task-status","title":"Available Task Status","text":"Name Value Description State IN_QUEUE IN_QUEUE Task is waiting in queue to be processed Waiting IN_PROGRESS IN_PROGRESS Task is currently being executed Active FINISHED FINISHED Task has been completed successfully Finished ERROR ERROR Task has failed with an error Error TIMEOUT TIMEOUT Task has timed out Error USER_CANCEL USER_CANCEL Task has been cancelled by user Stopped"},{"location":"description/netspresso/enums/task_status/#example","title":"Example","text":"<pre><code>from netspresso.enums import TaskStatus\n\n# Check task status\nstatus = TaskStatus.IN_PROGRESS\n\n# Available statuses\nprint(f\"In Queue: {TaskStatus.IN_QUEUE}\")\nprint(f\"In Progress: {TaskStatus.IN_PROGRESS}\")\nprint(f\"Finished: {TaskStatus.FINISHED}\")\nprint(f\"Error: {TaskStatus.ERROR}\")\nprint(f\"Timeout: {TaskStatus.TIMEOUT}\")\nprint(f\"User Cancel: {TaskStatus.USER_CANCEL}\")\n</code></pre>"},{"location":"description/netspresso/profiler/profiler/","title":"Profiler","text":""},{"location":"description/netspresso/profiler/profiler/#description","title":"Description","text":""},{"location":"description/netspresso/profiler/profiler/#netspresso.profiler.profiler.Profiler","title":"<code>netspresso.profiler.profiler.Profiler</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/profiler/profiler/#netspresso.profiler.profiler.Profiler.__init__","title":"<code>__init__(token_handler, user_info)</code>","text":"<p>Initialize the Profiler.</p>"},{"location":"description/netspresso/profiler/profiler/#netspresso.profiler.profiler.Profiler.cancel_profile_task","title":"<code>cancel_profile_task(profile_task_id)</code>","text":"<p>Cancel the profile task with given profile task uuid.</p> <p>Parameters:</p> Name Type Description Default <code>profile_task_id</code> <code>str</code> <p>Profile task UUID of the profile task.</p> required <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the task cancel.</p> <p>Returns:</p> Name Type Description <code>BenchmarkTask</code> <code>BenchmarkTask</code> <p>Model profile task dictionary.</p>"},{"location":"description/netspresso/profiler/profiler/#netspresso.profiler.profiler.Profiler.get_profile_task","title":"<code>get_profile_task(profile_task_id)</code>","text":"<p>Get information about the specified profile task using the profile task UUID.</p> <p>Parameters:</p> Name Type Description Default <code>profile_task_id</code> <code>str</code> <p>Profile task UUID of the profile task.</p> required <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs while retrieving information about the profile task.</p> <p>Returns:</p> Name Type Description <code>BenchmarkTask</code> <code>BenchmarkTask</code> <p>Model profile task object.</p>"},{"location":"description/netspresso/profiler/profiler/#netspresso.profiler.profiler.Profiler.profile_model","title":"<code>profile_model(input_model_path, target_device_name, target_software_version=None, target_hardware_type=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Profile the specified model on the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>target_device_name</code> <code>DeviceName</code> <p>Target device name.</p> required <code>target_software_version</code> <code>Union[str, SoftwareVersion]</code> <p>Target software version. Required if target_device_name is one of the Jetson devices.</p> <code>None</code> <code>target_hardware_type</code> <code>Union[str, HardwareType]</code> <p>Hardware type. Acceleration options for processing the model inference.</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, wait for the profile result before returning the function.                 If False, request the profile and return the function immediately.</p> <code>True</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the profiling of the model.</p> <p>Returns:</p> Name Type Description <code>ProfilerMetadata</code> <code>ProfilerMetadata</code> <p>Profiler metadata.</p>"},{"location":"description/netspresso/profiler/profiler/#examples","title":"Examples","text":""},{"location":"description/netspresso/profiler/profiler/#profiling-converted-model-on-jetson-agx-orin","title":"Profiling converted model on Jetson AGX Orin","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import DeviceName, SoftwareVersion\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nprofiler = netspresso.profiler()\nprofiling_task = profiler.profile_model(\n   input_model_path=\"./outputs/converted/TENSORRT_JETSON_AGX_ORIN_JETPACK_5_0_1/TENSORRT_JETSON_AGX_ORIN_JETPACK_5_0_1.trt\",\n   target_device_name=DeviceName.JETSON_AGX_ORIN,\n   target_software_version=SoftwareVersion.JETPACK_5_0_1,\n)\n</code></pre>"},{"location":"description/netspresso/quantizer/automatic_quantization/","title":"Quantizer(Automatic Quantization)","text":""},{"location":"description/netspresso/quantizer/automatic_quantization/#description","title":"Description","text":""},{"location":"description/netspresso/quantizer/automatic_quantization/#netspresso.quantizer.quantizer.Quantizer","title":"<code>netspresso.quantizer.quantizer.Quantizer</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/quantizer/automatic_quantization/#netspresso.quantizer.quantizer.Quantizer.automatic_quantization","title":"<code>automatic_quantization(input_model_path, output_dir, dataset_path, weight_precision=QuantizationPrecision.INT8, activation_precision=QuantizationPrecision.INT8, metric=SimilarityMetric.SNR, threshold=0, input_layers=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Apply automatic quantization to a model, specifying precision for weight &amp; activation.</p> <p>This method quantizes layers in the model based on the specified precision levels for weights and activations, while evaluating the quality of quantization using the defined metric. Only layers that meet the specified quality <code>threshold</code> are quantized; layers that do not meet this threshold remain unquantized to preserve model accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local folder path to save the quantized model.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Useful for certain quantizations.</p> required <code>weight_precision</code> <code>QuantizationPrecision</code> <p>Weight precision</p> <code>INT8</code> <code>activation_precision</code> <code>QuantizationPrecision</code> <p>Activation precision</p> <code>INT8</code> <code>metric</code> <code>SimilarityMetric</code> <p>Quantization quality metrics.</p> <code>SNR</code> <code>threshold</code> <code>Union[float, int]</code> <p>Quality threshold for quantization. Layers that do not meet this threshold based on the metric are not quantized.</p> <code>0</code> <code>input_layers</code> <code>List[InputShape]</code> <p>Target input shape for quantization (e.g., dynamic batch to static batch).</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, wait for the quantization result before returning the function.                 If False, request the quantization and return  the function immediately.</p> <code>True</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model quantization.</p> <p>Returns:</p> Name Type Description <code>QuantizerMetadata</code> <code>QuantizerMetadata</code> <p>Quantize metadata.</p>"},{"location":"description/netspresso/quantizer/automatic_quantization/#examples","title":"Examples","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import QuantizationPrecision\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nquantizer = netspresso.quantizer()\nquantization_result = quantizer.automatic_quantization(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/automatic_quantization\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    weight_precision=QuantizationPrecision.INT8,\n    activation_precision=QuantizationPrecision.INT8,\n    threshold=0,\n)\n</code></pre>"},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_layer_name/","title":"Quantizer(Custom Precision Quantization by Layer Name)","text":""},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_layer_name/#description","title":"Description","text":""},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_layer_name/#netspresso.quantizer.quantizer.Quantizer","title":"<code>netspresso.quantizer.quantizer.Quantizer</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_layer_name/#netspresso.quantizer.quantizer.Quantizer.custom_precision_quantization_by_layer_name","title":"<code>custom_precision_quantization_by_layer_name(input_model_path, output_dir, dataset_path, precision_by_layer_name, default_weight_precision=QuantizationPrecision.INT8, default_activation_precision=QuantizationPrecision.INT8, metric=SimilarityMetric.SNR, input_layers=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Apply custom precision quantization to a model, specifying precision for each layer name.</p> <p>This function allows precise control over the quantization process by enabling the user to specify quantization precision (e.g., INT8, FP16) for each named layer within the model. The <code>precision_by_layer_name</code> parameter provides a list where each item details the target precision for a specific layer name, enabling customized quantization that can enhance model performance or compatibility.</p> <p>Users can target specific layers to be quantized to lower precision for optimized model size and performance while keeping critical layers at higher precision for accuracy. Layers not explicitly listed in <code>precision_by_layer_name</code> will use <code>default_weight_precision</code> and <code>default_activation_precision</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local folder path to save the quantized model.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Useful for certain quantizations.</p> required <code>precision_by_layer_name</code> <code>List[PrecisionByLayer]</code> <p>List of <code>PrecisionByLayer</code> objects that specify the desired precision for each layer name in the model. Each entry includes: <code>name</code> (str): The layer name (e.g., /backbone/conv_first/block/act/Mul_output_0). <code>precision</code> (QuantizationPrecision): The quantization precision level.</p> required <code>default_weight_precision</code> <code>QuantizationPrecision</code> <p>Weight precision.</p> <code>INT8</code> <code>default_activation_precision</code> <code>QuantizationPrecision</code> <p>Activation precision.</p> <code>INT8</code> <code>metric</code> <code>SimilarityMetric</code> <p>Quantization quality metrics.</p> <code>SNR</code> <code>input_layers</code> <code>List[InputShape]</code> <p>Target input shape for quantization (e.g., dynamic batch to static batch).</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, wait for the quantization result before returning the function. If False, request the quantization and return immediately.</p> <code>True</code> <code>sleep_interval</code> <code>int</code> <p>Interval in seconds between checks when <code>wait_until_done</code> is True.</p> <code>30</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model quantization.</p> <p>Returns:</p> Name Type Description <code>QuantizerMetadata</code> <code>QuantizerMetadata</code> <p>Quantization metadata containing status, paths, etc.</p>"},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_layer_name/#examples","title":"Examples","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import QuantizationPrecision\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nquantizer = netspresso.quantizer()\n\nrecommendation_metadata = quantizer.get_recommendation_precision(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/automatic_quantization\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    weight_precision=QuantizationPrecision.INT8,\n    activation_precision=QuantizationPrecision.INT8,\n    threshold=0,\n)\nrecommendation_precisions = quantizer.load_recommendation_precision_result(recommendation_metadata.recommendation_result_path)\n\nquantization_result = quantizer.custom_precision_quantization_by_layer_name(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/custom_precision_quantization_by_layer_name\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    precision_by_layer_name=recommendation_precisions.layers,\n)\n</code></pre>"},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_operator_type/","title":"Quantizer(Custom Precision Quantization by Operator Type)","text":""},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_operator_type/#description","title":"Description","text":""},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_operator_type/#netspresso.quantizer.quantizer.Quantizer","title":"<code>netspresso.quantizer.quantizer.Quantizer</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_operator_type/#netspresso.quantizer.quantizer.Quantizer.custom_precision_quantization_by_operator_type","title":"<code>custom_precision_quantization_by_operator_type(input_model_path, output_dir, dataset_path, precision_by_operator_type, default_weight_precision=QuantizationPrecision.INT8, default_activation_precision=QuantizationPrecision.INT8, metric=SimilarityMetric.SNR, input_layers=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Apply custom quantization to a model, specifying precision for each operator type.</p> <p>This function allows for highly customizable quantization by enabling the user to specify the quantization precision (e.g., INT8, FP16) for each operator type within a model. The <code>precision_by_operator_type</code> parameter is a list of mappings where each entry indicates the quantization precision for a specific operator type, such as convolution (Conv), matrix multiplication (MatMul), etc.</p> <p>Using <code>precision_by_operator_type</code>, users can selectively fine-tune the quantization strategy for different operators within the model, based on performance requirements or hardware capabilities. Operators not explicitly specified in <code>precision_by_operator_type</code> will fall back to <code>default_weight_precision</code> and <code>default_activation_precision</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local folder path to save the quantized model.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Useful for certain quantizations.</p> required <code>precision_by_operator_type</code> <code>List[PrecisionByOperator]</code> <p>List of <code>PrecisionByOperator</code> objects that specify the desired precision for each operator type in the model. Each entry includes: <code>type</code> (str): The operator type (e.g., Conv, MatMul). <code>precision</code> (QuantizationPrecision): The quantization precision level.</p> required <code>default_weight_precision</code> <code>QuantizationPrecision</code> <p>Weight precision.</p> <code>INT8</code> <code>default_activation_precision</code> <code>QuantizationPrecision</code> <p>Activation precision.</p> <code>INT8</code> <code>metric</code> <code>SimilarityMetric</code> <p>Quantization quality metrics.</p> <code>SNR</code> <code>input_layers</code> <code>List[InputShape]</code> <p>Target input shape for quantization (e.g., dynamic batch to static batch).</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, wait for the quantization result before returning the function. If False, request the quantization and return immediately.</p> <code>True</code> <code>sleep_interval</code> <code>int</code> <p>Interval in seconds between checks when <code>wait_until_done</code> is True.</p> <code>30</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model quantization.</p> <p>Returns:</p> Name Type Description <code>QuantizerMetadata</code> <code>QuantizerMetadata</code> <p>Quantization metadata containing status, paths, etc.</p>"},{"location":"description/netspresso/quantizer/custom_precision_quantization_by_operator_type/#examples","title":"Examples","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import QuantizationPrecision\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nquantizer = netspresso.quantizer()\n\nrecommendation_metadata = quantizer.get_recommendation_precision(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/automatic_quantization\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    weight_precision=QuantizationPrecision.INT8,\n    activation_precision=QuantizationPrecision.INT8,\n    threshold=0,\n)\nrecommendation_precisions = quantizer.load_recommendation_precision_result(recommendation_metadata.recommendation_result_path)\n\nquantization_result = quantizer.custom_precision_quantization_by_operator_type(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/custom_precision_quantization_by_operator_type\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    precision_by_operator_type=recommendation_precisions.operators,\n)\n</code></pre>"},{"location":"description/netspresso/quantizer/recommendation_precision/","title":"Quantizer(Recommendation precision)","text":""},{"location":"description/netspresso/quantizer/recommendation_precision/#description","title":"Description","text":""},{"location":"description/netspresso/quantizer/recommendation_precision/#netspresso.quantizer.quantizer.Quantizer","title":"<code>netspresso.quantizer.quantizer.Quantizer</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/quantizer/recommendation_precision/#netspresso.quantizer.quantizer.Quantizer.get_recommendation_precision","title":"<code>get_recommendation_precision(input_model_path, output_dir, dataset_path, weight_precision=QuantizationPrecision.INT8, activation_precision=QuantizationPrecision.INT8, metric=SimilarityMetric.SNR, threshold=0, input_layers=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Get recommended precision for a model based on a specified quality threshold.</p> <p>This function analyzes each layer of the given model and recommends precision settings for layers that do not meet the specified threshold, helping to balance quantization quality and performance.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local folder path to save the quantized model.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Useful for certain quantizations.</p> required <code>weight_precision</code> <code>QuantizationPrecision</code> <p>Target precision for weights.</p> <code>INT8</code> <code>activation_precision</code> <code>QuantizationPrecision</code> <p>Target precision for activations.</p> <code>INT8</code> <code>metric</code> <code>SimilarityMetric</code> <p>Metric used to evaluate quantization quality.</p> <code>SNR</code> <code>threshold</code> <code>Union[float, int]</code> <p>Quality threshold; layers below this threshold will             receive precision recommendations.</p> <code>0</code> <code>input_layers</code> <code>List[Dict[str, int]]</code> <p>Specifications for input shapes             (e.g., to convert from dynamic to static batch size).</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, waits for the quantization process to finish             before returning. If False, starts the process and returns immediately.</p> <code>True</code> <code>sleep_interval</code> <code>int</code> <p>Interval, in seconds, between checks when <code>wait_until_done</code>             is True.</p> <code>30</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model quantization.</p> <p>Returns:</p> Name Type Description <code>QuantizerMetadata</code> <code>QuantizerMetadata</code> <p>Quantize metadata.</p>"},{"location":"description/netspresso/quantizer/recommendation_precision/#examples","title":"Examples","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import QuantizationPrecision\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nquantizer = netspresso.quantizer()\nrecommendation_metadata = quantizer.get_recommendation_precision(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/automatic_quantization\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    weight_precision=QuantizationPrecision.INT8,\n    activation_precision=QuantizationPrecision.INT8,\n    threshold=0,\n)\nrecommendation_precisions = quantizer.load_recommendation_precision_result(recommendation_metadata.recommendation_result_path)\n</code></pre>"},{"location":"description/netspresso/quantizer/uniform_precision_quantization/","title":"Quantizer(Uniform Precision Quantization)","text":""},{"location":"description/netspresso/quantizer/uniform_precision_quantization/#description","title":"Description","text":""},{"location":"description/netspresso/quantizer/uniform_precision_quantization/#netspresso.quantizer.quantizer.Quantizer","title":"<code>netspresso.quantizer.quantizer.Quantizer</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/quantizer/uniform_precision_quantization/#netspresso.quantizer.quantizer.Quantizer.uniform_precision_quantization","title":"<code>uniform_precision_quantization(input_model_path, output_dir, dataset_path, metric=SimilarityMetric.SNR, weight_precision=QuantizationPrecision.INT8, activation_precision=QuantizationPrecision.INT8, input_layers=None, wait_until_done=True, sleep_interval=30)</code>","text":"<p>Apply uniform precision quantization to a model, specifying precision for weight &amp; activation.</p> <p>This method quantizes all layers in the model uniformly based on the specified precision levels for weights and activations.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>str</code> <p>The file path where the model is located.</p> required <code>output_dir</code> <code>str</code> <p>The local folder path to save the quantized model.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Useful for certain quantizations.</p> required <code>metric</code> <code>SimilarityMetric</code> <p>Quantization quality metrics.</p> <code>SNR</code> <code>weight_precision</code> <code>QuantizationPrecision</code> <p>Weight precision</p> <code>INT8</code> <code>activation_precision</code> <code>QuantizationPrecision</code> <p>Activation precision</p> <code>INT8</code> <code>input_layers</code> <code>List[InputShape]</code> <p>Target input shape for quantization (e.g., dynamic batch to static batch).</p> <code>None</code> <code>wait_until_done</code> <code>bool</code> <p>If True, wait for the quantization result before returning the function.                 If False, request the quantization and return  the function immediately.</p> <code>True</code> <p>Raises:</p> Type Description <code>e</code> <p>If an error occurs during the model quantization.</p> <p>Returns:</p> Name Type Description <code>QuantizerMetadata</code> <p>Quantize metadata.</p>"},{"location":"description/netspresso/quantizer/uniform_precision_quantization/#examples","title":"Examples","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import QuantizationPrecision, SimilarityMetric\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\nquantizer = netspresso.quantizer()\nquantization_result = quantizer.uniform_precision_quantization(\n    input_model_path=\"./examples/sample_models/test.onnx\",\n    output_dir=\"./outputs/quantized/uniform_precision_quantization\",\n    dataset_path=\"./examples/sample_datasets/pickle_calibration_dataset_128x128.npy\",\n    metric=SimilarityMetric.SNR,\n    weight_precision=QuantizationPrecision.INT8,\n    activation_precision=QuantizationPrecision.INT8,\n)\n</code></pre>"},{"location":"description/netspresso/trainer/trainer/","title":"Trainer","text":""},{"location":"description/netspresso/trainer/trainer/#description","title":"Description","text":""},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer","title":"<code>netspresso.trainer.trainer.Trainer</code>","text":"<p>               Bases: <code>NetsPressoBase</code></p>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.__init__","title":"<code>__init__(token_handler, task=None, yaml_path=None)</code>","text":"<p>Initialize the Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Union[str, Task]]</code> <p>The type of task (classification, detection, segmentation). Either 'task' or 'yaml_path' must be provided, but not both.</p> <code>None</code> <code>yaml_path</code> <code>str</code> <p>Path to the YAML configuration file. Either 'task' or 'yaml_path' must be provided, but not both.</p> <code>None</code>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_augmentation_config","title":"<code>set_augmentation_config(train_transforms=None, inference_transforms=None)</code>","text":"<p>Set the augmentation configuration for training.</p> <p>Parameters:</p> Name Type Description Default <code>train_transforms</code> <code>List</code> <p>List of transforms for training. Defaults to None.</p> <code>None</code> <code>inference_transforms</code> <code>List</code> <p>List of transforms for inference. Defaults to None.</p> <code>None</code>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_dataset_config","title":"<code>set_dataset_config(name, root_path, train_image='images/train', train_label='labels/train', valid_image='images/valid', valid_label='labels/valid', test_image='images/valid', test_label='labels/valid', id_mapping=None)</code>","text":"<p>Set the dataset configuration for the Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of dataset.</p> required <code>root_path</code> <code>str</code> <p>Root directory of dataset.</p> required <code>train_image</code> <code>str</code> <p>The directory for training images. Should be relative path to root directory. Defaults to \"images/train\".</p> <code>'images/train'</code> <code>train_label</code> <code>str</code> <p>The directory for training labels. Should be relative path to root directory. Defaults to \"labels/train\".</p> <code>'labels/train'</code> <code>valid_image</code> <code>str</code> <p>The directory for validation images. Should be relative path to root directory. Defaults to \"images/val\".</p> <code>'images/valid'</code> <code>valid_label</code> <code>str</code> <p>The directory for validation labels. Should be relative path to root directory. Defaults to \"labels/val\".</p> <code>'labels/valid'</code> <code>id_mapping</code> <code>Union[List[str], Dict[str, str]]</code> <p>ID mapping for the dataset. Defaults to None.</p> <code>None</code>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_environment_config","title":"<code>set_environment_config(seed=1, num_workers=4)</code>","text":"<p>Set the environment configuration.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed. Defaults to 1.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>The number of multi-processing workers to be used by the data loader. Defaults to 4.</p> <code>4</code>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_fx_model","title":"<code>set_fx_model(fx_model_path)</code>","text":"<p>Set the FX model path for retraining.</p> <p>Parameters:</p> Name Type Description Default <code>fx_model_path</code> <code>str</code> <p>The path to the FX model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not set. Please use 'set_model_config' for model setup.</p>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_logging_config","title":"<code>set_logging_config(project_id=None, output_dir='./outputs', tensorboard=True, csv=False, image=True, stdout=True, save_optimizer_state=True, validation_epoch=10, save_checkpoint_epoch=None)</code>","text":"<p>Set the logging configuration.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>Project name to save the experiment. If None, it is set as {task}_{model} (e.g. segmentation_segformer).</p> <code>None</code> <code>output_dir</code> <code>str</code> <p>Root directory for saving the experiment. Defaults to \"./outputs\".</p> <code>'./outputs'</code> <code>tensorboard</code> <code>bool</code> <p>Whether to use the tensorboard. Defaults to True.</p> <code>True</code> <code>csv</code> <code>bool</code> <p>Whether to save the result in csv format. Defaults to False.</p> <code>False</code> <code>image</code> <code>bool</code> <p>Whether to save the validation results. It is ignored if the task is classification. Defaults to True.</p> <code>True</code> <code>stdout</code> <code>bool</code> <p>Whether to log the standard output. Defaults to True.</p> <code>True</code> <code>save_optimizer_state</code> <code>bool</code> <p>Whether to save optimizer state with model checkpoint to resume training. Defaults to True.</p> <code>True</code> <code>validation_epoch</code> <code>int</code> <p>Validation frequency in total training process. Defaults to 10.</p> <code>10</code> <code>save_checkpoint_epoch</code> <code>int</code> <p>Checkpoint saving frequency in total training process. Defaults to None.</p> <code>None</code>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_model_config","title":"<code>set_model_config(model_name, img_size, use_pretrained=True, load_head=False, path=None, fx_model_path=None, optimizer_path=None)</code>","text":"<p>Set the model configuration for the Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>img_size</code> <code>int</code> <p>Image size for the model.</p> required <code>use_pretrained</code> <code>bool</code> <p>Whether to use a pre-trained model. Defaults to True.</p> <code>True</code> <code>load_head</code> <code>bool</code> <p>Whether to load the model head. Defaults to False.</p> <code>False</code> <code>path</code> <code>str</code> <p>Path to the model. Defaults to None.</p> <code>None</code> <code>fx_model_path</code> <code>str</code> <p>Path to the FX model. Defaults to None.</p> <code>None</code> <code>optimizer_path</code> <code>str</code> <p>Path to the optimizer. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified model is not supported for the current task.</p>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.set_training_config","title":"<code>set_training_config(optimizer, scheduler, epochs=3, batch_size=8)</code>","text":"<p>Set the training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>The configuration of optimizer.</p> required <code>scheduler</code> <p>The configuration of learning rate scheduler.</p> required <code>epochs</code> <code>int</code> <p>The total number of epoch for training the model. Defaults to 3.</p> <code>3</code> <code>batch_size</code> <code>int</code> <p>The number of samples in single batch input. Defaults to 8.</p> <code>8</code>"},{"location":"description/netspresso/trainer/trainer/#netspresso.trainer.trainer.Trainer.train","title":"<code>train(gpus, project_name, output_dir='./outputs')</code>","text":"<p>Train the model with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>gpus</code> <code>str</code> <p>GPU ids to use, separated by commas.</p> required <code>project_name</code> <code>str</code> <p>Project name to save the experiment.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>TrainerMetadata</code> <p>A dictionary containing information about the training.</p>"},{"location":"description/netspresso/trainer/trainer/#examples","title":"Examples","text":""},{"location":"description/netspresso/trainer/trainer/#training","title":"Training","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.enums import Task\nfrom netspresso.trainer.augmentations import Resize\nfrom netspresso.trainer.optimizers import AdamW\nfrom netspresso.trainer.schedulers import CosineAnnealingWarmRestartsWithCustomWarmUp\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\n# 1. Declare trainer\ntrainer = netspresso.trainer(task=Task.OBJECT_DETECTION)\n\n# 2. Set config for training\n# 2-1. Data\ntrainer.set_dataset_config(\n   name=\"traffic_sign_config_example\",\n   root_path=\"/root/traffic-sign\",\n   train_image=\"images/train\",\n   train_label=\"labels/train\",\n   valid_image=\"images/valid\",\n   valid_label=\"labels/valid\",\n   id_mapping=[\"prohibitory\", \"danger\", \"mandatory\", \"other\"],\n)\n\n# 2-2. Model\nprint(trainer.available_models)  # ['EfficientFormer', 'YOLOX-S', ...]\ntrainer.set_model_config(model_name=\"YOLOX-S\", img_size=512)\n\n# 2-3. Augmentation\ntrainer.set_augmentation_config(\n   train_transforms=[Resize()],\n   inference_transforms=[Resize()],\n)\n\n# 2-4. Training\noptimizer = AdamW(lr=6e-3)\nscheduler = CosineAnnealingWarmRestartsWithCustomWarmUp(warmup_epochs=10)\ntrainer.set_training_config(\n   epochs=40,\n   batch_size=16,\n   optimizer=optimizer,\n   scheduler=scheduler,\n)\n\n# 3. Train\ntraining_result = trainer.train(gpus=\"0, 1\", project_name=\"project_sample\")\n</code></pre>"},{"location":"description/netspresso/trainer/trainer/#retraining","title":"Retraining","text":"<pre><code>from netspresso import NetsPresso\nfrom netspresso.trainer.optimizers import AdamW\n\n\nnetspresso = NetsPresso(email=\"YOUR_EMAIL\", password=\"YOUR_PASSWORD\")\n\n# 1. Declare trainer\ntrainer = netspresso.trainer(yaml_path=\"./temp/hparams.yaml\")\n\n# 2. Set config for retraining\n# 2-1. FX Model\ntrainer.set_fx_model(fx_model_path=\"./temp/FX_MODEL_PATH.pt\")\n\n# 2-2. Training\ntrainer.set_training_config(\n   epochs=30,\n   batch_size=16,\n   optimizer=AdamW(lr=6e-3),\n)\n\n# 3. Train\ntrainer.train(gpus=\"0, 1\", project_name=\"project_retrain_sample\")\n</code></pre>"},{"location":"description/netspresso_qai/base/dataset/get_dataset/","title":"Get Dataset","text":""},{"location":"description/netspresso_qai/base/dataset/get_dataset/#netspresso.np_qai.base.NPQAIBase.get_dataset","title":"<code>netspresso.np_qai.base.NPQAIBase.get_dataset(dataset_id)</code>","text":"<p>Get a dataset from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The ID of the dataset to get.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Returns a dataset object if successful.</p> Note <p>For details, see get_dataset in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/dataset/get_dataset/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\ndataset = np_qai.get_dataset(\"YOUR_DATASET_ID\")\n</code></pre>"},{"location":"description/netspresso_qai/base/dataset/get_datasets/","title":"Get Datasets","text":""},{"location":"description/netspresso_qai/base/dataset/get_datasets/#netspresso.np_qai.base.NPQAIBase.get_datasets","title":"<code>netspresso.np_qai.base.NPQAIBase.get_datasets(offset=0, limit=50)</code>","text":"<p>Get a list of datasets from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset of the datasets to get even older datasets.</p> <code>0</code> <code>limit</code> <code>int</code> <p>The limit of the datasets to get.</p> <code>50</code> <p>Returns:</p> Type Description <code>List[Dataset]</code> <p>List[Dataset]: Returns a list of dataset objects if successful.</p> Note <p>For details, see get_datasets in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/dataset/get_datasets/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\ndatasets = np_qai.get_datasets()\n</code></pre>"},{"location":"description/netspresso_qai/base/dataset/upload_dataset/","title":"Upload Dataset","text":""},{"location":"description/netspresso_qai/base/dataset/upload_dataset/#netspresso.np_qai.base.NPQAIBase.upload_dataset","title":"<code>netspresso.np_qai.base.NPQAIBase.upload_dataset(data, name=None)</code>","text":"<p>Upload a dataset to the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The dataset to upload.</p> required <code>name</code> <p>The name of the dataset.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Returns a dataset object if successful.</p> Note <p>For details, see upload_dataset in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/dataset/upload_dataset/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\ndataset = np_qai.upload_dataset(\"YOUR_DATASET_PATH\")\n</code></pre>"},{"location":"description/netspresso_qai/base/device/get_device_attributes/","title":"Get Device Attributes","text":""},{"location":"description/netspresso_qai/base/device/get_device_attributes/#netspresso.np_qai.base.NPQAIBase.get_device_attributes","title":"<code>netspresso.np_qai.base.NPQAIBase.get_device_attributes()</code>","text":"<p>Get a list of device attributes from the QAI Hub.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Returns a list of device attribute strings if successful.</p> Note <p>For details, see get_device_attributes in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/device/get_device_attributes/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\ndevice_attributes = np_qai.get_device_attributes()\n</code></pre>"},{"location":"description/netspresso_qai/base/device/get_devices/","title":"Get Devices","text":""},{"location":"description/netspresso_qai/base/device/get_devices/#netspresso.np_qai.base.NPQAIBase.get_devices","title":"<code>netspresso.np_qai.base.NPQAIBase.get_devices(name='', os='', attributes=None)</code>","text":"<p>Get a list of devices from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the device to get.</p> <code>''</code> <code>os</code> <code>str</code> <p>The OS of the device to get.</p> <code>''</code> <code>attributes</code> <code>Union[str, List[str]]</code> <p>The attributes of the device to get.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Device]</code> <p>List[Device]: Returns a list of device objects if successful.</p> Note <p>For details, see get_devices in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/device/get_devices/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\ndevices = np_qai.get_devices()\n</code></pre>"},{"location":"description/netspresso_qai/base/job/get_job/","title":"Get Job","text":""},{"location":"description/netspresso_qai/base/job/get_job/#netspresso.np_qai.base.NPQAIBase.get_job","title":"<code>netspresso.np_qai.base.NPQAIBase.get_job(job_id)</code>","text":"<p>Get a job from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The ID of the job to get.</p> required <p>Returns:</p> Name Type Description <code>Job</code> <code>Job</code> <p>Returns a job object if successful.</p> Note <p>For details, see get_job in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/job/get_job/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\njob = np_qai.get_job(\"YOUR_JOB_ID\")\n</code></pre>"},{"location":"description/netspresso_qai/base/job/get_job_summaries/","title":"Get Job Summaries","text":""},{"location":"description/netspresso_qai/base/job/get_job_summaries/#netspresso.np_qai.base.NPQAIBase.get_job_summaries","title":"<code>netspresso.np_qai.base.NPQAIBase.get_job_summaries(offset=0, limit=50, creator=None, state=None, type=None)</code>","text":"<p>Get a list of job summaries from the QAI Hub.</p> <p>Returns:</p> Type Description <code>List[JobSummary]</code> <p>List[JobSummary]: Returns a list of job summary objects if successful.</p> Note <p>For details, see get_job_summaries in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/job/get_job_summaries/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\njob_summaries = np_qai.get_job_summaries()\n</code></pre>"},{"location":"description/netspresso_qai/base/model/get_model/","title":"Get Model","text":""},{"location":"description/netspresso_qai/base/model/get_model/#netspresso.np_qai.base.NPQAIBase.get_model","title":"<code>netspresso.np_qai.base.NPQAIBase.get_model(model_id)</code>","text":"<p>Get a model from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model to get.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>Returns a model object if successful.</p> Note <p>For details, see get_model in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/model/get_model/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\nmodel = np_qai.get_model(\"YOUR_MODEL_ID\")\n</code></pre>"},{"location":"description/netspresso_qai/base/model/get_models/","title":"Get Models","text":""},{"location":"description/netspresso_qai/base/model/get_models/#netspresso.np_qai.base.NPQAIBase.get_models","title":"<code>netspresso.np_qai.base.NPQAIBase.get_models(offset=0, limit=50)</code>","text":"<p>Get a list of models from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset of the models to get even older models.</p> <code>0</code> <code>limit</code> <code>int</code> <p>The limit of the models to get.</p> <code>50</code> <p>Returns:</p> Type Description <code>List[Model]</code> <p>List[Model]: Returns a list of model objects if successful.</p> Note <p>For details, see get_models in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/model/get_models/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\nmodels = np_qai.get_models()\n</code></pre>"},{"location":"description/netspresso_qai/base/model/upload_model/","title":"Upload Model","text":""},{"location":"description/netspresso_qai/base/model/upload_model/#netspresso.np_qai.base.NPQAIBase.upload_model","title":"<code>netspresso.np_qai.base.NPQAIBase.upload_model(model, name=None)</code>","text":"<p>Upload a model to the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[SourceModel, str]</code> <p>The model to upload.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>Returns a model object if successful.</p> Note <p>For details, see upload_model in QAI Hub API.</p>"},{"location":"description/netspresso_qai/base/model/upload_model/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\nmodel = np_qai.upload_model(\"YOUR_MODEL_PATH\")\n</code></pre>"},{"location":"description/netspresso_qai/converter/convert_model/","title":"Convert Model","text":""},{"location":"description/netspresso_qai/converter/convert_model/#netspresso.np_qai.converter.NPQAIConverter.convert_model","title":"<code>netspresso.np_qai.converter.NPQAIConverter.convert_model(input_model_path, output_dir, target_device_name, input_shapes=None, options=CompileOptions(), job_name=None, single_compile=True, calibration_data=None, retry=True)</code>","text":"<p>Convert a model in the QAI hub.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>Union[str, Path]</code> <p>The path to the input model.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the converted model.</p> required <code>target_device_name</code> <code>Union[Device, List[Device]]</code> <p>The device to compile the model for.</p> required <code>input_shapes</code> <code>Optional[InputSpecs]</code> <p>The input shapes of the model.</p> <code>None</code> <code>options</code> <code>Union[CompileOptions, str]</code> <p>The options to use for the conversion.</p> <code>CompileOptions()</code> <code>job_name</code> <code>Optional[str]</code> <p>The name of the job.</p> <code>None</code> <code>single_compile</code> <code>bool</code> <p>Whether to compile the model in a single step.</p> <code>True</code> <code>calibration_data</code> <code>Union[Dataset, DatasetEntries, str, None]</code> <p>The calibration data to use for the conversion.</p> <code>None</code> <code>retry</code> <code>bool</code> <p>Whether to retry the conversion if it fails.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ConverterMetadata, List[ConverterMetadata]]</code> <p>Union[ConverterMetadata, List[ConverterMetadata]]: Returns a converter metadata object if successful.</p> Note <p>For details, see submit_compile_job in QAI Hub API.</p>"},{"location":"description/netspresso_qai/converter/convert_model/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\nfrom netspresso.np_qai import Device\nfrom netspresso.np_qai.options import CompileOptions, Runtime, ComputeUnit, QuantizeFullType\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\n\nconverter = np_qai.converter()\n\nconvert_options = CompileOptions(\n    target_runtime=Runtime.TFLITE,\n    compute_unit=[ComputeUnit.NPU],\n    quantize_full_type=QuantizeFullType.INT8,\n    quantize_io=True,\n    quantize_io_type=QuantizeFullType.INT8,\n)\n\nIMG_SIZE = 640\nINPUT_MODEL_PATH = \"YOUR_INPUT_MODEL_PATH\"\nOUTPUT_DIR = \"YOUR_OUTPUT_DIR\"\nJOB_NAME = \"YOUR_JOB_NAME\"\nDEVICE_NAME = \"QCS6490 (Proxy)\"\nconverted_result = converter.convert_model(\n    input_model_path=INPUT_MODEL_PATH,\n    output_dir=OUTPUT_DIR,\n    target_device_name=Device(DEVICE_NAME),\n    options=convert_options,\n    input_shapes=dict(image=(1, 3, IMG_SIZE, IMG_SIZE)),\n    job_name=JOB_NAME,\n)\n\nprint(\"Conversion task started\")\n\n# Monitor task status\nwhile True:\n    status = converter.get_convert_task_status(converted_result.convert_task_info.convert_task_uuid)\n    if status.finished:\n        converted_result = converter.update_convert_task(converted_result)\n        print(\"Conversion task completed\")\n        break\n    else:\n        print(\"Conversion task is still running\")\n</code></pre>"},{"location":"description/netspresso_qai/converter/download_model/","title":"Download Model","text":""},{"location":"description/netspresso_qai/converter/download_model/#netspresso.np_qai.converter.NPQAIConverter.download_model","title":"<code>netspresso.np_qai.converter.NPQAIConverter.download_model(job, filename)</code>","text":"<p>Download a model from the QAI Hub.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>CompileJob</code> <p>The job to download the model from.</p> required <code>filename</code> <code>str</code> <p>The filename to save the model to.</p> required Note <p>For details, see download_target_model in QAI Hub API.</p>"},{"location":"description/netspresso_qai/converter/get_conversion_task_status/","title":"Get Convert Task Status","text":""},{"location":"description/netspresso_qai/converter/get_conversion_task_status/#netspresso.np_qai.converter.NPQAIConverter.get_convert_task_status","title":"<code>netspresso.np_qai.converter.NPQAIConverter.get_convert_task_status(convert_task_id)</code>","text":"<p>Get the status of a convert task.</p> <p>Parameters:</p> Name Type Description Default <code>convert_task_id</code> <code>str</code> <p>The ID of the convert task to get the status of.</p> required <p>Returns:</p> Name Type Description <code>JobStatus</code> <code>JobStatus</code> <p>The status of the convert task.</p> Note <p>For details, see JobStatus in QAI Hub API.</p>"},{"location":"description/netspresso_qai/converter/update_conversion_task/","title":"Update Convert Task","text":""},{"location":"description/netspresso_qai/converter/update_conversion_task/#netspresso.np_qai.converter.NPQAIConverter.update_convert_task","title":"<code>netspresso.np_qai.converter.NPQAIConverter.update_convert_task(metadata)</code>","text":"<p>Update the convert task.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>ConverterMetadata</code> <p>The metadata of the convert task.</p> required <p>Returns:</p> Name Type Description <code>ConverterMetadata</code> <code>ConverterMetadata</code> <p>The updated metadata of the convert task.</p>"},{"location":"description/netspresso_qai/options/common/","title":"Common","text":""},{"location":"description/netspresso_qai/options/common/#netspresso.np_qai.options.common","title":"<code>netspresso.np_qai.options.common</code>","text":""},{"location":"description/netspresso_qai/options/common/#netspresso.np_qai.options.common.CommonOptions","title":"<code>CommonOptions</code>  <code>dataclass</code>","text":"<p>Common options for all tasks.</p> <p>Parameters:</p> Name Type Description Default <code>compute_unit</code> <code>Optional[List[ComputeUnit]]</code> <p>Specifies the target compute unit(s)</p> <code>None</code> Note <p>For details, see CommonOptions in QAI Hub API.</p>"},{"location":"description/netspresso_qai/options/compile/","title":"Compile","text":""},{"location":"description/netspresso_qai/options/compile/#netspresso.np_qai.options.compile","title":"<code>netspresso.np_qai.options.compile</code>","text":""},{"location":"description/netspresso_qai/options/compile/#netspresso.np_qai.options.compile.CompileOptions","title":"<code>CompileOptions</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CommonOptions</code></p> <p>Compile options for the model.</p> Note <p>For details, see CompileOptions in QAI Hub API.</p>"},{"location":"description/netspresso_qai/options/profile/","title":"Profile","text":""},{"location":"description/netspresso_qai/options/profile/#netspresso.np_qai.options.profile","title":"<code>netspresso.np_qai.options.profile</code>","text":""},{"location":"description/netspresso_qai/options/profile/#netspresso.np_qai.options.profile.InferenceOptions","title":"<code>InferenceOptions</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProfileCommonOptions</code></p> <p>Inference options for the model.</p> Note <p>For details, see InferenceOptions in QAI Hub API.</p>"},{"location":"description/netspresso_qai/options/profile/#netspresso.np_qai.options.profile.ProfileOptions","title":"<code>ProfileOptions</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProfileCommonOptions</code></p> <p>Profile options for the model.</p> Note <p>For details, see ProfileOptions in QAI Hub API.</p>"},{"location":"description/netspresso_qai/options/quantize/","title":"Quantize","text":""},{"location":"description/netspresso_qai/options/quantize/#netspresso.np_qai.options.quantize","title":"<code>netspresso.np_qai.options.quantize</code>","text":""},{"location":"description/netspresso_qai/options/quantize/#netspresso.np_qai.options.quantize.QuantizeOptions","title":"<code>QuantizeOptions</code>  <code>dataclass</code>","text":"<p>Quantize options for the model.</p> Note <p>For details, see QuantizeOptions in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/download_benchmark_results/","title":"Download Benchmark Results","text":""},{"location":"description/netspresso_qai/profiler/download_benchmark_results/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.download_benchmark_results","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.download_benchmark_results(job, artifacts_dir)</code>","text":"<p>Downloads the benchmark results from a given ProfileJob.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>ProfileJob</code> <p>The ProfileJob to download the results from.</p> required <code>artifacts_dir</code> <code>str</code> <p>The directory to save the results to.</p> required <p>Returns:</p> Name Type Description <code>ProfileJobResult</code> <code>ProfileJobResult</code> <p>The benchmark results.</p> Note <p>For details, see download_results in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/download_profile/","title":"Download Profile","text":""},{"location":"description/netspresso_qai/profiler/download_profile/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.download_profile","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.download_profile(job)</code>","text":"<p>Retrieves the profile data from the given ProfileJob.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>ProfileJob</code> <p>The ProfileJob to download the profile from.</p> required <p>Returns:</p> Type Description <p>The profile data.</p> Note <p>For details, see download_profile in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/get_benchmark_task_status/","title":"Get Benchmark Task Status","text":""},{"location":"description/netspresso_qai/profiler/get_benchmark_task_status/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.get_benchmark_task_status","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.get_benchmark_task_status(benchmark_task_id)</code>","text":"<p>Get the status of a benchmark task.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_task_id</code> <code>str</code> <p>The ID of the benchmark task to get the status of.</p> required <p>Returns:</p> Name Type Description <code>JobStatus</code> <code>JobStatus</code> <p>The status of the benchmark task.</p> Note <p>For details, see JobStatus in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/get_inference_task_status/","title":"Get Inference Task Status","text":""},{"location":"description/netspresso_qai/profiler/get_inference_task_status/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.get_inference_task_status","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.get_inference_task_status(inference_task_id)</code>","text":"<p>Get the status of an inference task.</p> <p>Parameters:</p> Name Type Description Default <code>inference_task_id</code> <code>str</code> <p>The ID of the inference task to get the status of.</p> required <p>Returns:</p> Name Type Description <code>JobStatus</code> <code>JobStatus</code> <p>The status of the inference task.</p> Note <p>For details, see JobStatus in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/inference_model/","title":"Inference Model","text":""},{"location":"description/netspresso_qai/profiler/inference_model/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.inference_model","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.inference_model(input_model_path, target_device_name, inputs, job_name=None, options=InferenceOptions(), retry=True)</code>","text":"<p>Inference a model on a device in the QAI hub.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>Union[str, Path]</code> <p>The path to the input model.</p> required <code>target_device_name</code> <code>Union[Device, List[Device]]</code> <p>The device to benchmark the model on.</p> required <code>inputs</code> <code>Union[Dataset, DatasetEntries, str]</code> <p>The input data to use for the inference.</p> required <code>job_name</code> <code>Optional[str]</code> <p>The name of the job.</p> <code>None</code> <code>options</code> <code>Union[InferenceOptions, str]</code> <p>The options to use for the inference.</p> <code>InferenceOptions()</code> <code>retry</code> <code>bool</code> <p>Whether to retry the inference if it fails.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[InferenceJob, List[InferenceJob]]</code> <p>Union[InferenceJob, List[InferenceJob]]: Returns an inference job object if successful.</p> Note <p>For details, see submit_inference_job in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/profile_model/","title":"Benchmark Model","text":""},{"location":"description/netspresso_qai/profiler/profile_model/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.benchmark_model","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.benchmark_model(input_model_path, target_device_name, options=ProfileOptions(), job_name=None, retry=True)</code>","text":"<p>Benchmark a model on a device in the QAI hub.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>Union[str, Path]</code> <p>The path to the input model.</p> required <code>target_device_name</code> <code>Union[Device, List[Device]]</code> <p>The device to benchmark the model on.</p> required <code>options</code> <code>Union[ProfileOptions, str]</code> <p>The options to use for the benchmark.</p> <code>ProfileOptions()</code> <code>job_name</code> <code>Optional[str]</code> <p>The name of the job.</p> <code>None</code> <code>retry</code> <code>bool</code> <p>Whether to retry the benchmark if it fails.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[BenchmarkerMetadata, List[BenchmarkerMetadata]]</code> <p>Union[BenchmarkerMetadata, List[BenchmarkerMetadata]]: Returns a benchmarker metadata object if successful.</p> Note <p>For details, see submit_profile_job in QAI Hub API.</p>"},{"location":"description/netspresso_qai/profiler/profile_model/#example","title":"Example","text":"<p>Visit your job on Qualcomm AI Hub to see other inference metrics like memory usage, load time, layer by layer analysis and model visualization.</p> <pre><code>from netspresso import NPQAI\nfrom netspresso.np_qai import Device\nfrom netspresso.np_qai.options import ProfileOptions, TfliteOptions, ComputeUnit\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\n\nbenchmarker = np_qai.benchmarker()\n\nINPUT_MODEL_PATH = \"YOUR_INPUT_MODEL_PATH\"\nOUTPUT_DIR = \"YOUR_OUTPUT_DIR\"\nJOB_NAME = \"YOUR_JOB_NAME\"\nDEVICE_NAME = \"QCS6490 (Proxy)\"\n\nbenchmark_options = ProfileOptions(\n    compute_unit=[ComputeUnit.NPU],\n    tflite_options=TfliteOptions(number_of_threads=4),\n)\n\nbenchmark_result = benchmarker.benchmark_model(\n    input_model_path=INPUT_MODEL_PATH,\n    target_device_name=Device(DEVICE_NAME),\n    options=benchmark_options,\n    job_name=JOB_NAME,\n)\n\n# Monitor task status\nwhile True:\n    status = benchmarker.get_benchmark_task_status(benchmark_result.benchmark_task_info.benchmark_task_uuid)\n    if status.finished:\n        benchmark_result = benchmarker.update_benchmark_task(benchmark_result)\n        print(\"Benchmark task completed\")\n        break\n    else:\n        print(\"Benchmark task is still running\")\n</code></pre>"},{"location":"description/netspresso_qai/profiler/update_benchmark_task/","title":"Update Benchmark Task","text":""},{"location":"description/netspresso_qai/profiler/update_benchmark_task/#netspresso.np_qai.benchmarker.NPQAIBenchmarker.update_benchmark_task","title":"<code>netspresso.np_qai.benchmarker.NPQAIBenchmarker.update_benchmark_task(metadata)</code>","text":"<p>Update the benchmark task.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>BenchmarkerMetadata</code> <p>The metadata of the benchmark task.</p> required <p>Returns:</p> Name Type Description <code>BenchmarkerMetadata</code> <code>BenchmarkerMetadata</code> <p>The updated metadata of the benchmark task.</p>"},{"location":"description/netspresso_qai/quantizer/download_model/","title":"Download Model","text":""},{"location":"description/netspresso_qai/quantizer/download_model/#netspresso.np_qai.quantizer.NPQAIQuantizer.download_model","title":"<code>netspresso.np_qai.quantizer.NPQAIQuantizer.download_model(job, filename)</code>","text":"<p>Download a model from the QAI hub.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>QuantizeJob</code> <p>The job to download the model from.</p> required <code>filename</code> <code>str</code> <p>The filename to save the model to.</p> required Note <p>For details, see download_target_model in QAI Hub API.</p>"},{"location":"description/netspresso_qai/quantizer/get_quantization_task_status/","title":"Get Quantize Task Status","text":""},{"location":"description/netspresso_qai/quantizer/get_quantization_task_status/#netspresso.np_qai.quantizer.NPQAIQuantizer.get_quantize_task_status","title":"<code>netspresso.np_qai.quantizer.NPQAIQuantizer.get_quantize_task_status(quantize_task_id)</code>","text":"<p>Get the status of a quantize task.</p> <p>Parameters:</p> Name Type Description Default <code>quantize_task_id</code> <code>str</code> <p>The ID of the quantize task to get the status of.</p> required <p>Returns:</p> Name Type Description <code>JobStatus</code> <code>JobStatus</code> <p>The status of the quantize task.</p> Note <p>For details, see JobStatus in QAI Hub API.</p>"},{"location":"description/netspresso_qai/quantizer/quantize_model/","title":"Quantize Model","text":""},{"location":"description/netspresso_qai/quantizer/quantize_model/#netspresso.np_qai.quantizer.NPQAIQuantizer.quantize_model","title":"<code>netspresso.np_qai.quantizer.NPQAIQuantizer.quantize_model(input_model_path, output_dir, weights_dtype, activations_dtype, options=QuantizeOptions(), job_name=None, calibration_data=None)</code>","text":"<p>Quantize a model in the QAI hub.</p> <p>Parameters:</p> Name Type Description Default <code>input_model_path</code> <code>Union[str, Path]</code> <p>The path to the input model.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the quantized model.</p> required <code>weights_dtype</code> <code>QuantizeDtype</code> <p>The data type to use for the weights.</p> required <code>activations_dtype</code> <code>QuantizeDtype</code> <p>The data type to use for the activations.</p> required <code>options</code> <code>Union[QuantizeOptions, str]</code> <p>The options to use for the quantization.</p> <code>QuantizeOptions()</code> <code>job_name</code> <code>Optional[str]</code> <p>The name of the job.</p> <code>None</code> <code>calibration_data</code> <code>Union[Dataset, DatasetEntries, str, None]</code> <p>The calibration data to use for the quantization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[NPQAIQuantizerMetadata, List[NPQAIQuantizerMetadata]]</code> <p>Union[NPQAIQuantizerMetadata, List[NPQAIQuantizerMetadata]]: Returns a quantizer metadata object if successful.</p> Note <p>For details, see submit_quantize_job in QAI Hub API.</p>"},{"location":"description/netspresso_qai/quantizer/quantize_model/#example","title":"Example","text":"<pre><code>from netspresso import NPQAI\nfrom netspresso.np_qai import Device\nfrom netspresso.np_qai.options import QuantizePrecision\n\nQAI_HUB_API_TOKEN = \"YOUR_QAI_HUB_API_TOKEN\"\nnp_qai = NPQAI(api_token=QAI_HUB_API_TOKEN)\n\nquantizer = np_qai.quantizer()\n\nINPUT_MODEL_PATH = \"YOUR_INPUT_MODEL_PATH\"\nOUTPUT_DIR = \"YOUR_OUTPUT_DIR\"\nJOB_NAME = \"YOUR_JOB_NAME\"\nDEVICE_NAME = \"YOUR_DEVICE_NAME\"\n\nCALIBRATION_DATA = {\"images\": inputs_array}\n\nquantized_result = quantizer.quantize_model(\n    input_model_path=INPUT_MODEL_PATH,\n    output_dir=OUTPUT_DIR,\n    calibration_data=CALIBRATION_DATA,\n    weights_dtype=QuantizePrecision.INT8,\n    activations_dtype=QuantizePrecision.INT8,\n    job_name=JOB_NAME,\n)\nprint(\"Quantization task started\")\n\n# Monitor task status\nwhile True:\n    status = quantizer.get_quantize_task_status(quantized_result.quantize_info.quantize_task_uuid)\n    if status.finished:\n        quantized_result = quantizer.update_quantize_task(quantized_result)\n        print(\"Quantization task completed\")\n        break\n    else:\n        print(\"Quantization task is still running\")\n</code></pre>"},{"location":"description/netspresso_qai/quantizer/quantize_model/#create-calibration-datasets","title":"Create Calibration Datasets","text":"<p>Using good, representative input samples for calibration helps improve performance on target hardware and retains model accuracy</p> <pre><code>import cv2\nfrom glob import glob\n\nimport numpy as np\nfrom netspresso.inferencer.preprocessors.base import Preprocessor\n\nIMG_SIZE = 640\npreprocess_list = [\n    {\n        \"name\": \"resize\",\n        \"size\": IMG_SIZE,\n        \"interpolation\": \"bilinear\",\n        \"max_size\": None,\n        \"resize_criteria\": \"long\",\n    },\n    {\n        \"name\": \"pad\",\n        \"size\": IMG_SIZE,\n        \"fill\": 114,\n    }\n]\n\npreprocessor = Preprocessor(preprocess_list)\n\nDATASET_PATH = \"YOUR_DATASET_PATH\"\nNUM_DATASET = 100\nimage_paths = glob(f\"{DATASET_PATH}/*.jpg\")[:NUM_DATASET]\n\ninputs_array = []\n\nfor image_path in image_paths:\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = preprocessor(img)\n    img = np.transpose(img, (0, 3, 1, 2))\n    inputs_array.append(img)\n\n&gt;&gt; np.array(inputs_array).shape\n(100, 1, 3, 512, 512)\n</code></pre>"},{"location":"description/netspresso_qai/quantizer/update_quantization_task/","title":"Update Quantize Task","text":""},{"location":"description/netspresso_qai/quantizer/update_quantization_task/#netspresso.np_qai.quantizer.NPQAIQuantizer.update_quantize_task","title":"<code>netspresso.np_qai.quantizer.NPQAIQuantizer.update_quantize_task(metadata)</code>","text":"<p>Update the quantize task.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>NPQAIQuantizerMetadata</code> <p>The metadata of the quantize task.</p> required <p>Returns:</p> Name Type Description <code>NPQAIQuantizerMetadata</code> <code>NPQAIQuantizerMetadata</code> <p>The updated metadata of the quantize task.</p>"}]}